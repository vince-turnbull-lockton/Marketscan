{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f44da14-36fd-4ee1-9403-f3c53bf647a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python \n",
    "\n",
    "#### Use this code to configure your accoutn before running SQL in Databricks\n",
    "\n",
    "storage_account_name = 'rgdevglobalreahl'\n",
    "storage_account_key =  'RjnFSNg2IDzQ5bN/8aTZOFK1lnpL6zytdMnuQIrJQMw6psa7++18fCoIfaLkq5DEFV3Hp0b7WZjR+AStQuhR+A=='\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col, lit, round \n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import expr, md5, concat_ws, coalesce, when\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e92e246-fed6-4808-b396-69f538a384a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####### Set Claims Data\n",
    "claims_source = \"wasbs://marketscan@rgdevglobalreahl.blob.core.windows.net/marketscan/medicaid/marketscan-preprod-step2\"\n",
    "df = spark.read.parquet(claims_source)\n",
    "\n",
    "\n",
    "### TOB mapping\n",
    "ip_tob_prefixes = [\"11\", \"12\", \"17\", \"21\", \"22\", \"81\", \"82\", \"84\", \"85\", \"86\"]\n",
    "snf_tob_prefixes = [\"17\", \"21\", \"22\"]\n",
    "hospice_tob_prefixes = [\"81\", \"82\", \"65\"]\n",
    "\n",
    "#### DRG Mapping (Upload after revising yearly)\n",
    "drg_mapping = pd.read_csv('DRG_Mapping.csv', dtype={\"DRG\": str})\n",
    "drg_map_spark = spark.createDataFrame(drg_mapping)\n",
    "drg_map_spark = drg_map_spark.withColumn(\"DRG\", col(\"DRG\").cast(\"string\"))\n",
    "\n",
    "#### BETOS Mapping (Upload after revising yearly) - for Prof Mapping HCPCS & CPT Codes\n",
    "betos_mapping = pd.read_csv('CMS_BETOS.csv', dtype={\"procedure_code\": str})\n",
    "betos_mapping_spark = spark.createDataFrame(betos_mapping)\n",
    "betos_mapping_spark = betos_mapping_spark.withColumn(\"procedure_code\", col(\"procedure_code\").cast(\"string\"))\n",
    "\n",
    "\n",
    "#########__________________Major Service Cat_____________________#####################\n",
    "\n",
    "\n",
    "### Major Service Category\n",
    "df = df.withColumn(\"major_service_category\", F.when((col(\"DRG\").isNull() & col(\"bill_type\").isNull()), \"Professional\").otherwise(\n",
    "    F.when(col(\"DRG\").isNotNull(), \"Inpatient Facility\").otherwise(F.when(F.col(\"bill_type\").substr(1, 2).isin(ip_tob_prefixes), \"Inpatient Facility\").otherwise(\"Outpatient Facility\"))))\n",
    "\n",
    "\n",
    "#########__________________IP Detailed Service Cat_____________________#####################\n",
    "#### Detailed Service Categories\n",
    "### IP Step 1: Hospice & SNF based on TOB\n",
    "df = df.withColumn(\"detailed_service_category_ip_HospSNF\", \n",
    "                   F.when((col(\"major_service_category\") == \"Inpatient Facility\") & (col(\"bill_type\").substr(1,2).isin(snf_tob_prefixes)), \"SNF\").otherwise(\n",
    "                       F.when((col(\"major_service_category\") == \"Inpatient Facility\") & (col(\"bill_type\").substr(1,2).isin(hospice_tob_prefixes)), \"Hospice\").otherwise(\n",
    "                        None)))\n",
    "\n",
    "### IP Step 2: Join with mapped DRGS\n",
    "df = df.join(\n",
    "    drg_map_spark.select(\"DRG\",\"Level_II\", \"Level_III\"),\n",
    "    on=\"DRG\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Use DRG mapping or previously determined categories\n",
    "df = df.withColumn(\"IP_level_II\", F.when(col(\"major_service_category\") == \"Inpatient Facility\", F.coalesce( col(\"detailed_service_category_ip_HospSNF\"), col(\"Level_II\"), lit(\"Other\"))).otherwise(None))\n",
    "df = df.withColumn(\"IP_level_III\", F.when(col(\"major_service_category\") == \"Inpatient Facility\", F.coalesce(col(\"detailed_service_category_ip_HospSNF\"), col(\"Level_III\"), lit(\"Other Inpatient Facility\"))).otherwise(None))\n",
    "\n",
    "df = df.drop(\"detailed_service_category_ip_HospSNF\", \"Level_II\", \"Level_III\")\n",
    "\n",
    "\n",
    "##########Note ... redo level II as level IV - more detail and map acute IP to level II ###########\n",
    "\n",
    "\n",
    "#########__________________Prof Detailed Service Cat_____________________#####################\n",
    "### Prof Step 1: Join the BETOS Mapping\n",
    "df = df.join(\n",
    "    betos_mapping_spark.select(\"procedure_code\",\"RBCS_Cat_Desc\", \"RBCS_SubCat_Desc\"),\n",
    "    on=\"procedure_code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "### Prof Step 2: Null out everything prof for level II and level III\n",
    "df = df.withColumn(\"Prof_level_II\", F.when(col(\"major_service_category\") == \"Professional\", F.coalesce(col(\"RBCS_Cat_Desc\"), lit(\"Other Professional\"))).otherwise(None))\n",
    "df = df.withColumn(\"Prof_level_III\", F.when(col(\"major_service_category\") == \"Professional\", F.coalesce(col(\"RBCS_SubCat_Desc\"), lit(\"Other Professional\"))).otherwise(None))\n",
    "\n",
    "df = df.drop(\"RBCS_Cat_Desc\", \"RBCS_SubCat_Desc\")\n",
    "\n",
    "#########__________________OP Detailed Service Cat_____________________#####################\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"OP_Level_III\",\n",
    "    expr(\"\"\"\n",
    "        CASE\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND bill_type LIKE '3%' THEN 'Home Health'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '037%' THEN 'Anesthesia'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '054%' THEN 'Ambulance'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '090%' THEN 'Behavioral Health'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '091%' THEN 'Behavioral Health'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '100%' THEN 'Behavioral Health'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '080%' THEN 'Dialysis'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '082%' THEN 'Dialysis'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '083%' THEN 'Dialysis'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '084%' THEN 'Dialysis'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '085%' THEN 'Dialysis'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '088%' THEN 'Dialysis'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '073%' THEN 'EKG/ECG/EEG'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '074%' THEN 'EKG/ECG/EEG'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '045%' THEN 'Emergency Room'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '025%' THEN 'Facility Dispensed Pharmacy'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '063%' THEN 'Facility Dispensed Pharmacy'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '089%' THEN 'Facility Dispensed Pharmacy'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '030%' THEN 'Lab/Pathology'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '031%' THEN 'Facility Dispensed Pharmacy'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '076%' THEN 'Observation'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '042%' THEN 'PT/OT/ST'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '043%' THEN 'PT/OT/ST'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '044%' THEN 'PT/OT/ST'\n",
    "\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '051%' THEN 'Outpatient Clinic'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '052%' THEN 'Outpatient Clinic'\n",
    "        \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '026%' THEN 'Outpatient IV Therapy'\n",
    "\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code in('0360','0361','0362','0367','0369','0481','0490', '0499', '0790', '0799')  THEN 'Outpatient Surgery'\n",
    "            \n",
    "            -- WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '087%' THEN 'Cell/Gene Therapy'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '032%' THEN 'Radiology - Diagnostic'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '033%' THEN 'Radiology - Therapeutic'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '035%' THEN 'Radiology - CT'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '061%' THEN 'Radiology - MRI/MRA/MRT'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '034%' THEN 'Radiology - Nuclear Medicine'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '040%' THEN 'Radiology - Other'\n",
    "\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '072%' THEN 'Labor and Delivery'\n",
    "\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '029%' THEN 'DME/Prosthetics/Supplies'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '027%' THEN 'DME/Prosthetics/Supplies'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '062%' THEN 'DME/Prosthetics/Supplies'\n",
    "\n",
    "            WHEN major_service_category = 'Outpatient Facility' THEN 'Other Outpatient Facility'\n",
    "            \n",
    "            ELSE NULL\n",
    "        END\n",
    "    \"\"\"))\n",
    "\n",
    "\n",
    "### V1 of detailed svc cat, will get recategorized later\n",
    "df = df.withColumn(\"detailed_service_category_temp\", expr(\"\"\"\n",
    "    case when major_service_category = 'Inpatient Facility' Then  IP_Level_III\n",
    "    when major_service_category = 'Outpatient Facility' Then  OP_Level_III\n",
    "    when major_service_category = 'Professional' Then  Prof_level_III\n",
    "    else NULL end\n",
    "    \"\"\"))\n",
    "\n",
    "\n",
    "### V1 of service cat ID, will get recategorized later\n",
    "df = df.withColumn(\n",
    "    \"hash_key_temp\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"detailed_service_category_temp\", \"Incurred_Date\"))\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "### Bill type 12X found to be re-billed as OP rules in an IP stay - this hierarchy ensures that the one billed with DRG comes first in the window function if there is a tie with incurred date\n",
    "df = df.withColumn(\"IP_DSC_Hierarchy\", expr(\"\"\"\n",
    "    case when IP_level_III = 'Other Inpatient Facility' Then  3\n",
    "    when IP_level_III in ('SNF', 'Hospice') Then  2\n",
    "    else 1 end\n",
    "    \"\"\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Write and Save\n",
    "target_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medical.parquet\" \n",
    "df.write.partitionBy(\"YEAR\", \"MS_Source_File\").parquet(target_path, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "734e5ff7-1f54-4ace-9a73-9337d0a568a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#########__________________Relabeling IP Adjacent Admissions (IP Acute)_____________________#####################\n",
    "\n",
    "\n",
    "####### Set Claims Data, Filter to IP Acute\n",
    "claims_source = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medical.parquet\" \n",
    "df = spark.read.parquet(claims_source)\n",
    "\n",
    "df = df.filter((col(\"major_service_category\") == \"Inpatient Facility\") & \n",
    "               (~col(\"IP_level_III\").isin([\"SNF\", \"Hospice\"])))\n",
    "\n",
    "\n",
    "### Inucrred date needs to be changed to admit date for SNF/Hospice\n",
    "# List of columns to group by\n",
    "\n",
    "#hash key temp is concat of mbr ID, detailed svc cat tmp, inc date\n",
    "\n",
    "\n",
    "dedup_df = df\n",
    "'''.select(\"member_id\", \"service_to_date\", \"incurred_date\", \"IP_level_II\", \"detailed_service_category_temp\",\"hash_key_temp\",\"major_service_category\", \"admit_date\" ,\"IP_DSC_Hierarchy\").distinct().orderBy(\"member_id\", \"incurred_date\")\n",
    "'''\n",
    "\n",
    "# Define window by member_id ordered by admit_date\n",
    "window_spec = Window.partitionBy(\"member_id\").orderBy(\"incurred_date\", \"IP_DSC_Hierarchy\", \"service_to_date\")\n",
    "\n",
    "# Add lagged discharge_date\n",
    "'''dedup_df = df.withColumn(\"prev_service_to_date\", F.lag(\"service_to_date\").over(window_spec)).withColumn(\"prev_IP_level_III\", F.lag(\"detailed_service_category_temp\").over(window_spec)).withColumn(\"prev_IP_level_II\", F.lag(\"IP_level_II\").over(window_spec)).withColumn(\"prev_hash_key\", F.lag(\"hash_key_temp\").over(window_spec))\n",
    "'''\n",
    "\n",
    "\n",
    "# Sort by incurred_date\n",
    "window_group = Window.partitionBy(\"member_id\").orderBy(\"incurred_date\",\"IP_DSC_Hierarchy\" )\n",
    "\n",
    "# Track running max of service_to_date\n",
    "## Becomes temp new discharge date & we 1st aggregate / regroup all svcs that fall under this\n",
    "dedup_df = dedup_df.withColumn(\"running_max_service_to\", F.max(\"service_to_date\").over(window_group))\n",
    "\n",
    "############### _________ handling overlapping IP _________________#####\n",
    "\n",
    "### create this key to handle overlapping IP svcs\n",
    "dedup_df = dedup_df.withColumn(\n",
    "    \"hash_key_temp_step2\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"running_max_service_to\"))\n",
    ")\n",
    "\n",
    "\n",
    "window_spec = Window.partitionBy(\"member_id\", \"hash_key_temp_step2\")\n",
    "\n",
    "### not used later??\n",
    "dedup_df = dedup_df.withColumn(\"min_incurred_date_over_hash_step2\", F.min(\"incurred_date\").over(window_spec))\n",
    "dedup_df = dedup_df.withColumn(\"min_IP_Hierarch_over_hash_step2\", F.min(\"IP_DSC_Hierarchy\").over(window_spec))\n",
    "\n",
    "\n",
    "\n",
    "#####___________________________\n",
    "\n",
    "#### do unique to handle multiple incurred dates and look at only one date at a time, not multiple ... for the purpose of then counting row numbers 1-1 with each encounter\n",
    "IP_svc_hash = dedup_df.select(\"hash_key_temp_step2\", \"member_id\", \"detailed_service_category_temp\", \"IP_level_II\", \"incurred_date\", \"IP_DSC_Hierarchy\").orderBy(\"incurred_date\", \"IP_DSC_Hierarchy\").distinct()\n",
    "\n",
    "\n",
    "### Keep on the 1st occuring value -- deduped\n",
    "window_spec = Window.partitionBy(\"hash_key_temp_step2\").orderBy(\"incurred_date\", \"IP_DSC_Hierarchy\")\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "IP_svc_hash = IP_svc_hash.filter(col(\"row_num\") == 1).distinct()\n",
    "\n",
    "\n",
    "dedup_df = dedup_df.join (\n",
    "    IP_svc_hash.select(\"hash_key_temp_step2\", \"detailed_service_category_temp\", \"IP_level_II\").withColumnRenamed(\"detailed_service_category_temp\", \"detailed_service_category_override\").withColumnRenamed(\"IP_level_II\", \"IP_level_II_override\"), \n",
    "    on=\"hash_key_temp_step2\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "### create new key to aggregate over - with revised svc categories after condensing overlapping dates by member\n",
    "dedup_df = dedup_df.withColumn(\n",
    "    \"svc_overide_hash\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"detailed_service_category_override\", \"min_incurred_date_over_hash_step2\")))\n",
    "\n",
    "\n",
    "############### _________ handling adjacent IP _________________#####\n",
    "\n",
    "dedup_df = dedup_df.withColumn(\n",
    "    \"hash_key_temp_step3\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"min_incurred_date_over_hash_step2\"))\n",
    ")\n",
    "\n",
    "IP_svc_hash = dedup_df.select(\"member_id\", \"running_max_service_to\", \"min_incurred_date_over_hash_step2\",\"hash_key_temp_step3\", \"detailed_service_category_override\", \"IP_level_II_override\", \"svc_overide_hash\" ).orderBy(\"incurred_date\").distinct()\n",
    "\n",
    "\n",
    "# Sort by Start_date\n",
    "window_spec = Window.partitionBy(\"member_id\").orderBy(\"min_incurred_date_over_hash_step2\")\n",
    "\n",
    "# Get previous end_date\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"prev_end\", F.lag(\"running_max_service_to\").over(window_spec))\n",
    "\n",
    "# Flag new episode when not overlapping or adjacent (i.e., Start_date > prev_end + 1)\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\n",
    "    \"new_episode_flag\",\n",
    "    F.when(\n",
    "        (F.col(\"prev_end\").isNull()) |\n",
    "        (F.datediff(\"min_incurred_date_over_hash_step2\", \"prev_end\") >= 1),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# Assign episode ID using cumulative sum\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"episode_id\", F.sum(\"new_episode_flag\").over(window_spec))\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"episode_id_hash\",  md5(concat_ws(\"||\", \"member_id\", \"episode_id\")))\n",
    "\n",
    "\n",
    "\n",
    "####__________________\n",
    "window_spec = Window.partitionBy(\"episode_id_hash\").orderBy(\"min_incurred_date_over_hash_step2\")\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"row_num_adjacent_IP\", F.row_number().over(window_spec))\n",
    "\n",
    "IP_svc_hash_dedup = IP_svc_hash.filter(col(\"row_num_adjacent_IP\") == 1).distinct()\n",
    "\n",
    "IP_svc_hash_dedup = IP_svc_hash_dedup.select(\"member_id\", \"detailed_service_category_override\", \"IP_level_II_override\",\"episode_id_hash\", \"svc_overide_hash\")\n",
    "\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.join (\n",
    "    IP_svc_hash_dedup.select( \"episode_id_hash\", \"detailed_service_category_override\", \"IP_level_II_override\").withColumnRenamed(\"detailed_service_category_override\", \"detailed_service_category_override_final\").withColumnRenamed(\"IP_level_II_override\", \"IP_level_II_override_final\"), \n",
    "    on=\"episode_id_hash\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "### Calc final admit / discharge\n",
    "window_spec = Window.partitionBy(\"episode_id_hash\")\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"final_IP_admit\", F.min(\"min_incurred_date_over_hash_step2\").over(window_spec))\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"final_IP_discharge\", F.max(\"running_max_service_to\").over(window_spec))\n",
    "\n",
    "#### join back to full claims\n",
    "#### LEFT OFF CHECK HERE --- svc hash formbr vs. claims\n",
    "\n",
    "\n",
    "dedup_df = dedup_df.join(\n",
    "    IP_svc_hash.select(\"svc_overide_hash\", \"detailed_service_category_override_final\", \"IP_level_II_override_final\", \"final_IP_admit\", \"final_IP_discharge\"),\n",
    "    on=\"svc_overide_hash\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "### drop extra columns\n",
    "\n",
    "dedup_df = dedup_df.drop(\"hash_key_temp_step3\", \"IP_level_II_override\", \"detailed_service_category_override\", \"min_IP_Hierarch_over_hash_step2\", \"min_incurred_date_over_hash_step2\", \"running_max_service_to\", \"hash_key_temp_step2\",\"svc_overide_hash\", \"detailed_service_category_temp\", \"hash_key_temp\", \"detailed_service_category_temp\thash_key_temp\tIP_DSC_Hierarchy\")\n",
    "\n",
    "\n",
    "## clean up steps\n",
    "### there is a very small amount of rows that sometime get categorized as NULL -- not sure why; but it was ~ < 0.1% of IP acute medical spend after runnning a number of times. \n",
    "dedup_df = (\n",
    "    dedup_df\n",
    "    .withColumn(\n",
    "        \"detailed_service_category_override_final\",\n",
    "        coalesce(col(\"detailed_service_category_override_final\"), lit(\"Medical\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"IP_level_II_override_final\",\n",
    "        coalesce(col(\"IP_level_II_override_final\"), lit(\"Ungrouped\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"final_IP_admit\",\n",
    "        coalesce(col(\"final_IP_admit\"), col(\"incurred_date\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"final_IP_discharge\",\n",
    "        coalesce(col(\"final_IP_discharge\"), col(\"service_to_date\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"IP_detailed_svccat_errorflag\",\n",
    "        when(col(\"detailed_service_category_override_final\").isNull(), lit(True)).otherwise(lit(False))\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "target_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medclaims_IPAcute.parquet\" \n",
    "dedup_df.write.parquet(target_path, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8daf5c6-ba56-46c5-9af4-e244dfdede81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#########__________________Relabeling IP Adjacent Admissions (SNF)_____________________#####################\n",
    "\n",
    "\n",
    "####### Set Claims Data, Filter to IP Acute\n",
    "claims_source = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medical.parquet\" \n",
    "df = spark.read.parquet(claims_source)\n",
    "\n",
    "df = df.filter((col(\"major_service_category\") == \"Inpatient Facility\") & \n",
    "               (col(\"IP_level_III\").isin([\"SNF\"])))\n",
    "\n",
    "\n",
    "### Inucrred date needs to be changed to admit date for SNF/Hospice\n",
    "# List of columns to group by\n",
    "\n",
    "#hash key temp is concat of mbr ID, detailed svc cat tmp, inc date\n",
    "\n",
    "\n",
    "dedup_df = df\n",
    "'''.select(\"member_id\", \"service_to_date\", \"incurred_date\", \"IP_level_II\", \"detailed_service_category_temp\",\"hash_key_temp\",\"major_service_category\", \"admit_date\" ,\"IP_DSC_Hierarchy\").distinct().orderBy(\"member_id\", \"incurred_date\")\n",
    "'''\n",
    "\n",
    "# Define window by member_id ordered by admit_date\n",
    "window_spec = Window.partitionBy(\"member_id\").orderBy(\"incurred_date\", \"IP_DSC_Hierarchy\", \"service_to_date\")\n",
    "\n",
    "# Add lagged discharge_date\n",
    "'''dedup_df = df.withColumn(\"prev_service_to_date\", F.lag(\"service_to_date\").over(window_spec)).withColumn(\"prev_IP_level_III\", F.lag(\"detailed_service_category_temp\").over(window_spec)).withColumn(\"prev_IP_level_II\", F.lag(\"IP_level_II\").over(window_spec)).withColumn(\"prev_hash_key\", F.lag(\"hash_key_temp\").over(window_spec))\n",
    "'''\n",
    "\n",
    "\n",
    "# Sort by incurred_date\n",
    "window_group = Window.partitionBy(\"member_id\").orderBy(\"incurred_date\",\"IP_DSC_Hierarchy\" )\n",
    "\n",
    "# Track running max of service_to_date\n",
    "dedup_df = dedup_df.withColumn(\"running_max_service_to\", F.max(\"service_to_date\").over(window_group))\n",
    "\n",
    "############### _________ handling overlapping IP _________________#####\n",
    "dedup_df = dedup_df.withColumn(\n",
    "    \"hash_key_temp_step2\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"running_max_service_to\"))\n",
    ")\n",
    "\n",
    "\n",
    "window_spec = Window.partitionBy(\"member_id\", \"hash_key_temp_step2\")\n",
    "\n",
    "dedup_df = dedup_df.withColumn(\"min_incurred_date_over_hash_step2\", F.min(\"incurred_date\").over(window_spec))\n",
    "\n",
    "dedup_df = dedup_df.withColumn(\"min_IP_Hierarch_over_hash_step2\", F.min(\"IP_DSC_Hierarchy\").over(window_spec))\n",
    "\n",
    "\n",
    "\n",
    "#####___________________________\n",
    "\n",
    "IP_svc_hash = dedup_df.select(\"hash_key_temp_step2\", \"member_id\", \"detailed_service_category_temp\", \"IP_level_II\", \"incurred_date\", \"IP_DSC_Hierarchy\").orderBy(\"incurred_date\", \"IP_DSC_Hierarchy\").distinct()\n",
    "\n",
    "window_spec = Window.partitionBy(\"hash_key_temp_step2\").orderBy(\"incurred_date\", \"IP_DSC_Hierarchy\")\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.filter(col(\"row_num\") == 1).distinct()\n",
    "\n",
    "\n",
    "dedup_df = dedup_df.join (\n",
    "    IP_svc_hash.select(\"hash_key_temp_step2\", \"detailed_service_category_temp\", \"IP_level_II\").withColumnRenamed(\"detailed_service_category_temp\", \"detailed_service_category_override\").withColumnRenamed(\"IP_level_II\", \"IP_level_II_override\"), \n",
    "    on=\"hash_key_temp_step2\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "dedup_df = dedup_df.withColumn(\n",
    "    \"svc_overide_hash\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"detailed_service_category_override\", \"min_incurred_date_over_hash_step2\")))\n",
    "\n",
    "\n",
    "############### _________ handling adjacent IP _________________#####\n",
    "\n",
    "dedup_df = dedup_df.withColumn(\n",
    "    \"hash_key_temp_step3\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"min_incurred_date_over_hash_step2\"))\n",
    ")\n",
    "\n",
    "IP_svc_hash = dedup_df.select(\"member_id\", \"running_max_service_to\", \"min_incurred_date_over_hash_step2\",\"hash_key_temp_step3\", \"detailed_service_category_override\", \"IP_level_II_override\", \"svc_overide_hash\" ).orderBy(\"incurred_date\").distinct()\n",
    "\n",
    "\n",
    "# Sort by Start_date\n",
    "window_spec = Window.partitionBy(\"member_id\").orderBy(\"min_incurred_date_over_hash_step2\")\n",
    "\n",
    "# Get previous end_date\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"prev_end\", F.lag(\"running_max_service_to\").over(window_spec))\n",
    "\n",
    "# Flag new episode when not overlapping or adjacent (i.e., Start_date > prev_end + 1)\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\n",
    "    \"new_episode_flag\",\n",
    "    F.when(\n",
    "        (F.col(\"prev_end\").isNull()) |\n",
    "        (F.datediff(\"min_incurred_date_over_hash_step2\", \"prev_end\") >= 1),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# Assign episode ID using cumulative sum\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"episode_id\", F.sum(\"new_episode_flag\").over(window_spec))\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"episode_id_hash\",  md5(concat_ws(\"||\", \"member_id\", \"episode_id\")))\n",
    "\n",
    "\n",
    "####__________________\n",
    "window_spec = Window.partitionBy(\"episode_id_hash\").orderBy(\"min_incurred_date_over_hash_step2\")\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"row_num_adjacent_IP\", F.row_number().over(window_spec))\n",
    "\n",
    "IP_svc_hash_dedup = IP_svc_hash.filter(col(\"row_num_adjacent_IP\") == 1).distinct()\n",
    "IP_svc_hash_dedup = IP_svc_hash_dedup.select(\"member_id\", \"detailed_service_category_override\", \"IP_level_II_override\",\"episode_id_hash\", \"svc_overide_hash\")\n",
    "\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.join (\n",
    "    IP_svc_hash_dedup.select( \"episode_id_hash\", \"detailed_service_category_override\", \"IP_level_II_override\").withColumnRenamed(\"detailed_service_category_override\", \"detailed_service_category_override_final\").withColumnRenamed(\"IP_level_II_override\", \"IP_level_II_override_final\"), \n",
    "    on=\"episode_id_hash\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "### Calc final admit / discharge\n",
    "window_spec = Window.partitionBy(\"episode_id_hash\")\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"final_IP_admit\", F.min(\"min_incurred_date_over_hash_step2\").over(window_spec))\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"final_IP_discharge\", F.max(\"running_max_service_to\").over(window_spec))\n",
    "\n",
    "dedup_df = dedup_df.join(\n",
    "    IP_svc_hash.select(\"svc_overide_hash\", \"detailed_service_category_override_final\", \"IP_level_II_override_final\", \"final_IP_admit\", \"final_IP_discharge\"),\n",
    "    on=\"svc_overide_hash\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "### drop extra columns\n",
    "dedup_df = dedup_df.drop(\"hash_key_temp_step3\", \"IP_level_II_override\", \"detailed_service_category_override\", \"min_IP_Hierarch_over_hash_step2\", \"min_incurred_date_over_hash_step2\", \"running_max_service_to\", \"hash_key_temp_step2\",\"svc_overide_hash\", \"detailed_service_category_temp\", \"hash_key_temp\", \"detailed_service_category_temp\thash_key_temp\tIP_DSC_Hierarchy\")\n",
    "\n",
    "\n",
    "## clean up steps\n",
    "### there is a very small amount of rows that sometime get categorized as NULL -- not sure why; but it was ~ < 0.1% of IP acute medical spend after runnning a number of times. \n",
    "dedup_df = (\n",
    "    dedup_df\n",
    "    .withColumn(\n",
    "        \"detailed_service_category_override_final\",\n",
    "        coalesce(col(\"detailed_service_category_override_final\"), lit(\"Medical\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"IP_level_II_override_final\",\n",
    "        coalesce(col(\"IP_level_II_override_final\"), lit(\"Ungrouped\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"final_IP_admit\",\n",
    "        coalesce(col(\"final_IP_admit\"), col(\"incurred_date\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"final_IP_discharge\",\n",
    "        coalesce(col(\"final_IP_discharge\"), col(\"service_to_date\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"IP_detailed_svccat_errorflag\",\n",
    "        when(col(\"detailed_service_category_override_final\").isNull(), lit(True)).otherwise(lit(False))\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "target_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medclaims_SNF.parquet\" \n",
    "dedup_df.write.parquet(target_path, mode=\"overwrite\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e20828b-d353-46b1-9811-8a0f2047f201",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#########__________________Relabeling IP Adjacent Admissions (Hospice)_____________________#####################\n",
    "\n",
    "\n",
    "####### Set Claims Data, Filter to IP Acute\n",
    "claims_source = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medical.parquet\" \n",
    "df = spark.read.parquet(claims_source)\n",
    "\n",
    "df = df.filter((col(\"major_service_category\") == \"Inpatient Facility\") & \n",
    "               (col(\"IP_level_III\").isin([\"Hospice\"])))\n",
    "\n",
    "\n",
    "### Inucrred date needs to be changed to admit date for SNF/Hospice\n",
    "# List of columns to group by\n",
    "\n",
    "#hash key temp is concat of mbr ID, detailed svc cat tmp, inc date\n",
    "\n",
    "\n",
    "dedup_df = df\n",
    "'''.select(\"member_id\", \"service_to_date\", \"incurred_date\", \"IP_level_II\", \"detailed_service_category_temp\",\"hash_key_temp\",\"major_service_category\", \"admit_date\" ,\"IP_DSC_Hierarchy\").distinct().orderBy(\"member_id\", \"incurred_date\")\n",
    "'''\n",
    "\n",
    "# Define window by member_id ordered by admit_date\n",
    "window_spec = Window.partitionBy(\"member_id\").orderBy(\"incurred_date\", \"IP_DSC_Hierarchy\", \"service_to_date\")\n",
    "\n",
    "# Add lagged discharge_date\n",
    "'''dedup_df = df.withColumn(\"prev_service_to_date\", F.lag(\"service_to_date\").over(window_spec)).withColumn(\"prev_IP_level_III\", F.lag(\"detailed_service_category_temp\").over(window_spec)).withColumn(\"prev_IP_level_II\", F.lag(\"IP_level_II\").over(window_spec)).withColumn(\"prev_hash_key\", F.lag(\"hash_key_temp\").over(window_spec))\n",
    "'''\n",
    "\n",
    "\n",
    "# Sort by incurred_date\n",
    "window_group = Window.partitionBy(\"member_id\").orderBy(\"incurred_date\",\"IP_DSC_Hierarchy\" )\n",
    "\n",
    "# Track running max of service_to_date\n",
    "dedup_df = dedup_df.withColumn(\"running_max_service_to\", F.max(\"service_to_date\").over(window_group))\n",
    "\n",
    "############### _________ handling overlapping IP _________________#####\n",
    "dedup_df = dedup_df.withColumn(\n",
    "    \"hash_key_temp_step2\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"running_max_service_to\"))\n",
    ")\n",
    "\n",
    "\n",
    "window_spec = Window.partitionBy(\"member_id\", \"hash_key_temp_step2\")\n",
    "\n",
    "dedup_df = dedup_df.withColumn(\"min_incurred_date_over_hash_step2\", F.min(\"incurred_date\").over(window_spec))\n",
    "\n",
    "dedup_df = dedup_df.withColumn(\"min_IP_Hierarch_over_hash_step2\", F.min(\"IP_DSC_Hierarchy\").over(window_spec))\n",
    "\n",
    "\n",
    "\n",
    "#####___________________________\n",
    "\n",
    "IP_svc_hash = dedup_df.select(\"hash_key_temp_step2\", \"member_id\", \"detailed_service_category_temp\", \"IP_level_II\", \"incurred_date\", \"IP_DSC_Hierarchy\").orderBy(\"incurred_date\", \"IP_DSC_Hierarchy\").distinct()\n",
    "\n",
    "window_spec = Window.partitionBy(\"hash_key_temp_step2\").orderBy(\"incurred_date\", \"IP_DSC_Hierarchy\")\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.filter(col(\"row_num\") == 1).distinct()\n",
    "\n",
    "\n",
    "dedup_df = dedup_df.join (\n",
    "    IP_svc_hash.select(\"hash_key_temp_step2\", \"detailed_service_category_temp\", \"IP_level_II\").withColumnRenamed(\"detailed_service_category_temp\", \"detailed_service_category_override\").withColumnRenamed(\"IP_level_II\", \"IP_level_II_override\"), \n",
    "    on=\"hash_key_temp_step2\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "dedup_df = dedup_df.withColumn(\n",
    "    \"svc_overide_hash\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"detailed_service_category_override\", \"min_incurred_date_over_hash_step2\")))\n",
    "\n",
    "\n",
    "############### _________ handling adjacent IP _________________#####\n",
    "\n",
    "dedup_df = dedup_df.withColumn(\n",
    "    \"hash_key_temp_step3\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"min_incurred_date_over_hash_step2\"))\n",
    ")\n",
    "\n",
    "IP_svc_hash = dedup_df.select(\"member_id\", \"running_max_service_to\", \"min_incurred_date_over_hash_step2\",\"hash_key_temp_step3\", \"detailed_service_category_override\", \"IP_level_II_override\", \"svc_overide_hash\" ).orderBy(\"incurred_date\").distinct()\n",
    "\n",
    "\n",
    "# Sort by Start_date\n",
    "window_spec = Window.partitionBy(\"member_id\").orderBy(\"min_incurred_date_over_hash_step2\")\n",
    "\n",
    "# Get previous end_date\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"prev_end\", F.lag(\"running_max_service_to\").over(window_spec))\n",
    "\n",
    "# Flag new episode when not overlapping or adjacent (i.e., Start_date > prev_end + 1)\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\n",
    "    \"new_episode_flag\",\n",
    "    F.when(\n",
    "        (F.col(\"prev_end\").isNull()) |\n",
    "        (F.datediff(\"min_incurred_date_over_hash_step2\", \"prev_end\") >= 1),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# Assign episode ID using cumulative sum\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"episode_id\", F.sum(\"new_episode_flag\").over(window_spec))\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"episode_id_hash\",  md5(concat_ws(\"||\", \"member_id\", \"episode_id\")))\n",
    "\n",
    "\n",
    "####__________________\n",
    "window_spec = Window.partitionBy(\"episode_id_hash\").orderBy(\"min_incurred_date_over_hash_step2\")\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"row_num_adjacent_IP\", F.row_number().over(window_spec))\n",
    "\n",
    "IP_svc_hash_dedup = IP_svc_hash.filter(col(\"row_num_adjacent_IP\") == 1).distinct()\n",
    "IP_svc_hash_dedup = IP_svc_hash_dedup.select(\"member_id\", \"detailed_service_category_override\", \"IP_level_II_override\",\"episode_id_hash\", \"svc_overide_hash\")\n",
    "\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.join (\n",
    "    IP_svc_hash_dedup.select( \"episode_id_hash\", \"detailed_service_category_override\", \"IP_level_II_override\").withColumnRenamed(\"detailed_service_category_override\", \"detailed_service_category_override_final\").withColumnRenamed(\"IP_level_II_override\", \"IP_level_II_override_final\"), \n",
    "    on=\"episode_id_hash\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "### Calc final admit / discharge\n",
    "window_spec = Window.partitionBy(\"episode_id_hash\")\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"final_IP_admit\", F.min(\"min_incurred_date_over_hash_step2\").over(window_spec))\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"final_IP_discharge\", F.max(\"running_max_service_to\").over(window_spec))\n",
    "\n",
    "dedup_df = dedup_df.join(\n",
    "    IP_svc_hash.select(\"svc_overide_hash\", \"detailed_service_category_override_final\", \"IP_level_II_override_final\", \"final_IP_admit\", \"final_IP_discharge\"),\n",
    "    on=\"svc_overide_hash\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "### drop extra columns\n",
    "dedup_df = dedup_df.drop(\"hash_key_temp_step3\", \"IP_level_II_override\", \"detailed_service_category_override\", \"min_IP_Hierarch_over_hash_step2\", \"min_incurred_date_over_hash_step2\", \"running_max_service_to\", \"hash_key_temp_step2\",\"svc_overide_hash\", \"detailed_service_category_temp\", \"hash_key_temp\", \"detailed_service_category_temp\thash_key_temp\tIP_DSC_Hierarchy\")\n",
    "\n",
    "\n",
    "## clean up steps\n",
    "### there is a very small amount of rows that sometime get categorized as NULL -- not sure why; but it was ~ < 0.1% of IP acute medical spend after runnning a number of times. \n",
    "dedup_df = (\n",
    "    dedup_df\n",
    "    .withColumn(\n",
    "        \"detailed_service_category_override_final\",\n",
    "        coalesce(col(\"detailed_service_category_override_final\"), lit(\"Medical\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"IP_level_II_override_final\",\n",
    "        coalesce(col(\"IP_level_II_override_final\"), lit(\"Ungrouped\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"final_IP_admit\",\n",
    "        coalesce(col(\"final_IP_admit\"), col(\"incurred_date\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"final_IP_discharge\",\n",
    "        coalesce(col(\"final_IP_discharge\"), col(\"service_to_date\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"IP_detailed_svccat_errorflag\",\n",
    "        when(col(\"detailed_service_category_override_final\").isNull(), lit(True)).otherwise(lit(False))\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "target_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medclaims_Hospice.parquet\" \n",
    "dedup_df.write.parquet(target_path, mode=\"overwrite\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f038fda-b23a-4890-aa81-b2caae0398b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Union the IP Tables; Add Util count field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "475e57f1-ff0b-42f7-a80d-5dd2c05cb6b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hospice = spark.read.parquet(\"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medclaims_Hospice.parquet\")\n",
    "\n",
    "df_snf = spark.read.parquet(\"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medclaims_SNF.parquet\")\n",
    "\n",
    "df_ipacute = spark.read.parquet(\"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medclaims_IPAcute.parquet\")\n",
    "\n",
    "union_df = df_hospice.unionByName(df_snf).unionByName(df_ipacute)\n",
    "\n",
    "\n",
    "union_df = union_df.withColumn(\n",
    "    \"service_cat_id\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"detailed_service_category_override_final\", \"final_IP_admit\")))\n",
    "\n",
    "\n",
    "union_df = union_df.withColumn(\"svc_cat_id_row_num\", F.row_number().over(Window.partitionBy(\"service_cat_id\").orderBy(\"claim_number\", \"seqnum\")))\n",
    "\n",
    "window_spec = Window.partitionBy(\"service_cat_id\")\n",
    "\n",
    "union_df = union_df.withColumn(\"util_count\", \n",
    "                              F.when((F.col(\"svc_cat_id_row_num\") == 1) & (F.sum(\"allowed\").over(window_spec) > 0), 1)\n",
    "                              .when((F.col(\"svc_cat_id_row_num\") == 1) & (F.sum(\"allowed\").over(window_spec) < 0), -1)\n",
    "                              .when((F.col(\"svc_cat_id_row_num\") == 1) & (F.sum(\"allowed\").over(window_spec) == 0), 0)\n",
    "                              .otherwise(None))\n",
    "\n",
    "union_df = union_df.drop(\"Prof_level_II\", \"Prof_level_III\", \"OP_Level_III\", \"IP_DSC_Hierarchy\", \"svc_cat_id_row_num\", \"IP_level_II\", \"IP_level_III\").withColumnRenamed(\"detailed_service_category_override_final\", \"detailed_service_category\").withColumnRenamed(\"IP_level_II_override_final\", \"IP_admit_clinical_label\").withColumnRenamed(\"final_IP_admit\", \"lockton_admit\").withColumnRenamed(\"final_IP_discharge\", \"lockton_discharge\")\n",
    "\n",
    "union_df = union_df.withColumn(\"subservice_category\", \n",
    "                              F.when((F.col(\"detailed_service_category\") == \"SNF\"), \"SNF\")\n",
    "                              .when((F.col(\"detailed_service_category\") == \"Hospice\"), \"Hospice\")\n",
    "                              .otherwise(\"Acute\"))\n",
    "\n",
    "\n",
    "\n",
    "########### Test\n",
    "\n",
    "target_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medclaims_IP_Split.parquet\"\n",
    "union_df_IP = union_df.write.partitionBy(\"YEAR\", \"MS_Source_File\").parquet(target_path, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf353a1f-e518-4b7a-bc8d-ce0a7ad0eb2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "OP Hierarchy and Observation Mapping; Add Util Field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffc70d0e-438e-4981-bb49-969241dcc8f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####### Set Claims Data, Filter to OP Facility\n",
    "claims_source = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medical.parquet\" \n",
    "df = spark.read.parquet(claims_source)\n",
    "\n",
    "\n",
    "############### Logic for everything but HH -- HH happens outside of a OP facility event (ex. ER admit leading surgery for example)\n",
    "\n",
    "df = df.filter((col(\"major_service_category\") == \"Outpatient Facility\") & \n",
    "               (~col(\"OP_level_III\").isin([\"Home Health\"])))\n",
    "\n",
    "df = df.withColumn(\"OP_Hierarchy\", \n",
    "                   when(col(\"OP_level_III\") == \"Outpatient Surgery\", 1)\n",
    "                   .when(col(\"OP_level_III\") == \"Observation\", 2)\n",
    "                   .when(col(\"OP_level_III\") == \"Emergency Room\", 3)\n",
    "                   .otherwise(4))\n",
    "\n",
    "window_spec = Window.partitionBy(\"incurred_date\", \"member_id\")\n",
    "df = df.withColumn(\"Min_Hierarchy\", F.min(\"OP_Hierarchy\").over(window_spec))\n",
    "\n",
    "df = df.withColumn(\"Remap_OP\",\n",
    "                   when (col(\"Min_Hierarchy\") == 1, \"Outpatient Surgery\")\n",
    "                   .when (col(\"Min_Hierarchy\") == 2, \"Observation\")\n",
    "                   .when (col(\"Min_Hierarchy\") == 3, \"Emergency Room\")\n",
    "                   .otherwise(col(\"OP_level_III\")))\n",
    "\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"incurred_date_hash\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"incurred_date\")))\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"service_to_date_hash\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"service_to_date\")))\n",
    "\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"incurred_date_hash_svc\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"Remap_OP\", \"incurred_date\")))\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"service_to_date_hash_svc\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"Remap_OP\", \"service_to_date\")))\n",
    "\n",
    "\n",
    "##### multi day observation happens iwth rev vode 762 -- allow for service cat ID to be defined by grouping across days if multi day observation\n",
    "multiday_obs = df.filter(col(\"revenue_code\") == '0762').select(\"member_id\", \"revenue_code\", \"service_to_date_hash_svc\").withColumn(\"multiday_obs_flag\", lit(1)).distinct()\n",
    "\n",
    "##### define svc cat ID here - based on incurred date and remapped svc category\n",
    "df = df.join(\n",
    "    multiday_obs.select(\"service_to_date_hash_svc\", \"multiday_obs_flag\"),\n",
    "    on=\"service_to_date_hash_svc\",\n",
    "    how=\"left\"\n",
    ").withColumn(\n",
    "    \"service_cat_id\",\n",
    "    when(\n",
    "        col(\"multiday_obs_flag\").isNotNull(),\n",
    "        col(\"service_to_date_hash_svc\")\n",
    "    ).otherwise(col(\"incurred_date_hash_svc\"))\n",
    ")\n",
    "\n",
    "\n",
    "##### svc cat ID defined, so now can calculate lockton admit and discharge date (controlling definition here) -- I.e. OP facility by def has 1 date of service, except multiday observation; in final step allowing for HH to be across dates too by letting it's service to date flow through \n",
    "\n",
    "### create lockton admit and discharge\n",
    "window_spec = Window.partitionBy(\"service_cat_id\")\n",
    "df = df.withColumn(\"lockton_admit\", F.min(\"incurred_date\").over(window_spec))\n",
    "df = df.withColumn(\"lockton_discharge\", F.max(\"service_to_date\").over(window_spec))\n",
    "df = df.withColumnRenamed(\"Remap_OP\", \"detailed_service_category_final\")\n",
    "df_OP = df\n",
    "\n",
    "\n",
    "################## _______________ OP HH _______________ #####\n",
    "df_hh = spark.read.parquet(claims_source)\n",
    "\n",
    "df_hh = df_hh.filter((col(\"major_service_category\") == \"Outpatient Facility\") & \n",
    "               (col(\"OP_level_III\").isin([\"Home Health\"])))\n",
    "\n",
    "df_hh = df_hh.withColumn(\"lockton_admit\", coalesce(col(\"admit_date\"), col(\"incurred_date\")))\n",
    "df_hh = df_hh.withColumn(\"lockton_discharge\", coalesce(col(\"discharge_date\"), col(\"incurred_date\")))\n",
    "df_hh = df_hh.withColumnRenamed(\"OP_level_III\", \"detailed_service_category_final\")\n",
    "\n",
    "\n",
    "df_hh = df_hh.withColumn(\n",
    "    \"service_cat_id\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"detailed_service_category_final\", \"lockton_admit\")))\n",
    "\n",
    "union_df_OP = df_OP.unionByName(df_hh, allowMissingColumns=True)\n",
    "\n",
    "\n",
    "union_df_OP = union_df_OP.withColumn(\"svc_cat_id_row_num\", F.row_number().over(Window.partitionBy(\"service_cat_id\").orderBy(\"claim_number\", \"seqnum\")))\n",
    "\n",
    "window_spec = Window.partitionBy(\"service_cat_id\")\n",
    "\n",
    "union_df_OP = union_df_OP.withColumn(\"util_count\", \n",
    "                              F.when((F.col(\"svc_cat_id_row_num\") == 1) & (F.sum(\"allowed\").over(window_spec) > 0), 1)\n",
    "                              .when((F.col(\"svc_cat_id_row_num\") == 1) & (F.sum(\"allowed\").over(window_spec) < 0), -1)\n",
    "                              .when((F.col(\"svc_cat_id_row_num\") == 1) & (F.sum(\"allowed\").over(window_spec) == 0), 0)\n",
    "                              .otherwise(None))\n",
    "\n",
    "\n",
    "\n",
    "union_df_OP = union_df_OP.drop(\"service_to_date_hash_svc\", \"IP_level_II\", \"IP_level_III\", \"Prof_level_II\", \"Prof_level_III\", \"OP_Level_III\", \"detailed_service_category_temp\", \"hash_key_temp\", \"IP_DSC_Hierarchy\", \"OP_Hierarchy\", \"Min_Hierarchy\", \"incurred_date_hash\", \"service_to_date_hash\", \"incurred_date_hash_svc\", \"svc_cat_id_row_num\").withColumnRenamed(\"detailed_service_category_final\", \"detailed_service_category\")\n",
    "\n",
    "\n",
    "\n",
    "union_df_OP = union_df_OP.withColumn(\"subservice_category\", \n",
    "                              F.when((col(\"detailed_service_category\").isin([\"Emergency Room\", \"Outpatient Surgery\", \"Observation\"])), \"OP Visit\")\n",
    "                              .otherwise(\"OP Service\"))\n",
    "\n",
    "\n",
    "\n",
    "##### Export\n",
    "target_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medclaims_OP_Split.parquet\"\n",
    "\n",
    "union_df_OP.write.partitionBy(\"YEAR\", \"MS_Source_File\").parquet(target_path, mode=\"overwrite\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d54ce09-ddd1-4b8e-bc36-f5f6d62edd60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Professional Charges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1245d5d-2977-495a-9d95-66ed9cc3de75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####### Set Claims Data, Filter to OP Facility\n",
    "claims_source = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medical.parquet\" \n",
    "df = spark.read.parquet(claims_source)\n",
    "\n",
    "\n",
    "############### Logic for everything but HH -- HH happens outside of a OP facility event (ex. ER admit leading surgery for example)\n",
    "\n",
    "df_prof = df.filter((col(\"major_service_category\") == \"Professional\"))\n",
    "                    \n",
    "\n",
    "### add svc cat id after renaming DSC, add util count\n",
    "df_prof = df_prof.withColumnRenamed(\"hash_key_temp\", \"service_cat_id\").withColumnRenamed(\"detailed_service_category_temp\", \"detailed_service_category\").withColumnRenamed(\"Prof_level_II\", \"subservice_category\")\n",
    "\n",
    "df_prof = df_prof.withColumn(\"svc_cat_id_row_num\", F.row_number().over(Window.partitionBy(\"service_cat_id\").orderBy(\"claim_number\", \"seqnum\")))\n",
    "\n",
    "window_spec = Window.partitionBy(\"service_cat_id\")\n",
    "\n",
    "df_prof = df_prof.withColumn(\"util_count\", \n",
    "                              F.when((F.col(\"svc_cat_id_row_num\") == 1) & (F.sum(\"allowed\").over(window_spec) > 0), 1)\n",
    "                              .when((F.col(\"svc_cat_id_row_num\") == 1) & (F.sum(\"allowed\").over(window_spec) < 0), -1)\n",
    "                              .when((F.col(\"svc_cat_id_row_num\") == 1) & (F.sum(\"allowed\").over(window_spec) == 0), 0)\n",
    "                              .otherwise(None))\n",
    "\n",
    "\n",
    "df_prof = df_prof.drop(\"IP_level_II\", \"IP_level_III\", \"Prof_level_III\", \"OP_Level_III\", \"IP_DSC_Hierarchy\", \"svc_cat_id_row_num\")\n",
    "                    \n",
    "##### Export\n",
    "target_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medclaims_Prof_Split.parquet\"\n",
    "\n",
    "df_prof.write.partitionBy(\"YEAR\", \"MS_Source_File\").parquet(target_path, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d236ac15-dfd8-49da-bb46-debf17bfd121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Final Combination of the Tables and Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15761ddc-d457-4a57-a86f-6a5a9a338be1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "IP_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medclaims_IP_Split.parquet\"\n",
    "OP_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medclaims_OP_Split.parquet\"\n",
    "Prof_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medclaims_Prof_Split.parquet\"\n",
    "\n",
    "\n",
    "IP = spark.read.parquet(IP_path)\n",
    "OP = spark.read.parquet(OP_path)\n",
    "Prof = spark.read.parquet(Prof_path)\n",
    "\n",
    "Combined_table = IP.unionByName(OP, allowMissingColumns=True).unionByName(Prof, allowMissingColumns=True)\n",
    "\n",
    "\n",
    "# Define the desired order of columns\n",
    "\n",
    "IP_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medclaims_IP_Split.parquet\"\n",
    "OP_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medclaims_OP_Split.parquet\"\n",
    "Prof_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medclaims_Prof_Split.parquet\"\n",
    "\n",
    "\n",
    "IP = spark.read.parquet(IP_path)\n",
    "OP = spark.read.parquet(OP_path)\n",
    "Prof = spark.read.parquet(Prof_path)\n",
    "\n",
    "Combined_table = IP.unionByName(OP, allowMissingColumns=True).unionByName(Prof, allowMissingColumns=True)\n",
    "\n",
    "'''\n",
    "# Define the desired order of columns\n",
    "column_order = [\"procedure_code\",\n",
    "\"DRG\",\n",
    "\"fachdid\",\n",
    "\"member_id\",\n",
    "\"claim_number\",\n",
    "\"seqnum\",\n",
    "\"marketscan_version_number\",\n",
    "\"family_id\",\n",
    "\"principal_diagnosis\",\n",
    "\"icd_diagnosis_code1\",\n",
    "\"icd_diagnosis_code2\",\n",
    "\"icd_diagnosis_code3\",\n",
    "\"icd_diagnosis_code4\",\n",
    "\"icd_diagnosis_code5\",\n",
    "\"icd_diagnosis_code6\",\n",
    "\"icd_diagnosis_code7\",\n",
    "\"icd_diagnosis_code8\",\n",
    "\"icd_diagnosis_code9\",\n",
    "\"principal_procedure\",\n",
    "\"procedure_code2\",\n",
    "\"procedure_code3\",\n",
    "\"procedure_code4\",\n",
    "\"procedure_code5\",\n",
    "\"procedure_code6\",\n",
    "\"cpt_modifier\",\n",
    "\"bill_type\",\n",
    "\"admit_type\",\n",
    "\"discharge_status\",\n",
    "\"revenue_code\",\n",
    "\"cob\",\n",
    "\"coins\",\n",
    "\"copay\",\n",
    "\"deduct\",\n",
    "\"paid\",\n",
    "\"allowed\",\n",
    "\"quantity\",\n",
    "\"units\",\n",
    "\"paid_inn\",\n",
    "\"provider_id\",\n",
    "\"npi\",\n",
    "\"service_to_date\",\n",
    "\"paid_date\",\n",
    "\"incurred_date\",\n",
    "\"discharge_date\",\n",
    "\"admit_date\",\n",
    "\"dob_year\",\n",
    "\"member_age\",\n",
    "\"cap_svc\",\n",
    "\"proctyp\",\n",
    "\"dxver\",\n",
    "\"facprof\",\n",
    "\"mhsacovg\",\n",
    "\"ntwkprov\",\n",
    "\"plan_type\",\n",
    "\"proc_group\",\n",
    "\"service_category\",\n",
    "\"MDC\",\n",
    "\"employee_region\",\n",
    "\"employee_msa\",\n",
    "\"pos\",\n",
    "\"provider_type\",\n",
    "\"data_type\",\n",
    "\"age_group\",\n",
    "\"employee_class\",\n",
    "\"employee_status\",\n",
    "\"employee_geo\",\n",
    "\"eidflag\",\n",
    "\"employee_relation\",\n",
    "\"enrflag\",\n",
    "\"physician_specialty\",\n",
    "\"rx\",\n",
    "\"member_gender\",\n",
    "\"healthplan_vs_employer\",\n",
    "\"industry\",\n",
    "\"medicare_advantage_flag\",\n",
    "\"msn_version\",\n",
    "\"caseid\",\n",
    "\"major_service_category\",\n",
    "\"subservice_category\",\n",
    "\"detailed_service_category\",\n",
    "\"service_cat_id\",\n",
    "\"util_count\",\n",
    "\"IP_admit_clinical_label\",\n",
    "\"lockton_admit\",\n",
    "\"lockton_discharge\",\n",
    "\"IP_detailed_svccat_errorflag\",\n",
    "\"multiday_obs_flag\",\n",
    "\"YEAR\",\n",
    "\"MS_Source_File\"]\n",
    "\n",
    "\n",
    "\n",
    "# Reorder the columns in Combined_table\n",
    "Combined_table = Combined_table.select([col(c) for c in column_order])\n",
    "'''\n",
    "\n",
    "\n",
    "target_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical_final/ms_medicaid_medclaims_combined.parquet\"\n",
    "\n",
    "Combined_table.write.partitionBy(\"YEAR\", \"MS_Source_File\").parquet(target_path, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02705b91-d9a7-46f7-93e2-6d1378d302ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rx Mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ddbbbe0-5d69-45e6-8399-21151e6054ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Rx_path = \"wasbs://marketscan@rgdevglobalreahl.blob.core.windows.net/marketscan/medicaid/marketscan-Rx-preprod-step2\"\n",
    "Rx = spark.read.parquet(Rx_path)\n",
    "\n",
    "Rx = Rx.withColumn(\"major_service_category\", lit(\"Rx\"))\n",
    "\n",
    "#### mail order ind not available on the medicaid file -- so use days supply as a proxy\n",
    "Rx = Rx.withColumn(\n",
    "    \"subservice_category\",\n",
    "    F.when(F.abs(col(\"allowed\")) > 2500, \"Specialty\")\n",
    "    .when(F.abs(col(\"days_supply\")) >= 84, \"Mail Order\")\n",
    "    .otherwise(\"Retail\"))\n",
    "\n",
    "\n",
    "Rx = Rx.withColumn(\n",
    "    \"detailed_service_category\",\n",
    "    F.when(F.abs(col(\"allowed\")) > 2500, \"Specialty\")\n",
    "    .when(col(\"generic_equivalent\").isin([\"1\"]), \"Single-Source Brand\")\n",
    "    .when(col(\"generic_equivalent\").isin([\"2\", \"3\"]), \"Multi-Source Brand\")\n",
    "    .when(col(\"generic_equivalent\").isin([\"4\", \"5\"]), \"Generic\")\n",
    "    .otherwise(\"OTC/Other\"))\n",
    "\n",
    "\n",
    "target_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/Rx/ms_medicaid_Rxclaims_combined.parquet\"\n",
    "Rx.write.partitionBy(\"year\" ).parquet(target_path, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0a714dc-1bdc-47b5-8683-700e8b0ce2ef",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"#row_number#\":52},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752090065378}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3451601888346498>, line 121\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m column_order \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprocedure_code\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDRG\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     30\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfachdid\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    115\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYEAR\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    116\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMS_Source_File\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\u001B[1;32m    120\u001B[0m \u001B[38;5;66;03m# Reorder the columns in Combined_table\u001B[39;00m\n",
       "\u001B[0;32m--> 121\u001B[0m Combined_table \u001B[38;5;241m=\u001B[39m Combined_table\u001B[38;5;241m.\u001B[39mselect([col(c) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m column_order])\n",
       "\u001B[1;32m    125\u001B[0m target_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medclaims_combined.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    127\u001B[0m Combined_table\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYEAR\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMS_Source_File\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mparquet(target_path, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3853\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   3808\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   3809\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   3810\u001B[0m \n",
       "\u001B[1;32m   3811\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3851\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n",
       "\u001B[1;32m   3852\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3853\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jcols(\u001B[38;5;241m*\u001B[39mcols))\n",
       "\u001B[1;32m   3854\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `family_id` cannot be resolved. Did you mean one of the following? [`allowed`, `fachdid`, `member_id`, `prov_id`, `admit_type`]. SQLSTATE: 42703;\n",
       "'Project [procedure_code#60768, DRG#60769, fachdid#60770, member_id#60771, claim_number#60772, seqnum#60773, marketscan_version_number#60774, 'family_id, principal_diagnosis#60775, icd_diagnosis_code1#60776, icd_diagnosis_code2#60777, icd_diagnosis_code3#60778, icd_diagnosis_code4#60779, icd_diagnosis_code5#60780, icd_diagnosis_code6#60781, icd_diagnosis_code7#60782, icd_diagnosis_code8#60783, icd_diagnosis_code9#60784, principal_procedure#60785, procedure_code2#60786, procedure_code3#60787, procedure_code4#60788, procedure_code5#60789, procedure_code6#60790, ... 65 more fields]\n",
       "+- Union false, false\n",
       "   :- Project [procedure_code#60768, DRG#60769, fachdid#60770, member_id#60771, claim_number#60772, seqnum#60773, marketscan_version_number#60774, principal_diagnosis#60775, icd_diagnosis_code1#60776, icd_diagnosis_code2#60777, icd_diagnosis_code3#60778, icd_diagnosis_code4#60779, icd_diagnosis_code5#60780, icd_diagnosis_code6#60781, icd_diagnosis_code7#60782, icd_diagnosis_code8#60783, icd_diagnosis_code9#60784, principal_procedure#60785, procedure_code2#60786, procedure_code3#60787, procedure_code4#60788, procedure_code5#60789, procedure_code6#60790, cpt_modifier#60791, ... 53 more fields]\n",
       "   :  +- Relation [procedure_code#60768,DRG#60769,fachdid#60770,member_id#60771,claim_number#60772,seqnum#60773,marketscan_version_number#60774,principal_diagnosis#60775,icd_diagnosis_code1#60776,icd_diagnosis_code2#60777,icd_diagnosis_code3#60778,icd_diagnosis_code4#60779,icd_diagnosis_code5#60780,icd_diagnosis_code6#60781,icd_diagnosis_code7#60782,icd_diagnosis_code8#60783,icd_diagnosis_code9#60784,principal_procedure#60785,procedure_code2#60786,procedure_code3#60787,procedure_code4#60788,procedure_code5#60789,procedure_code6#60790,cpt_modifier#60791,... 52 more fields] parquet\n",
       "   :- Project [procedure_code#60920, DRG#60921, fachdid#60922, member_id#60923, claim_number#60924, seqnum#60925, marketscan_version_number#60926, principal_diagnosis#60927, icd_diagnosis_code1#60928, icd_diagnosis_code2#60929, icd_diagnosis_code3#60930, icd_diagnosis_code4#60931, icd_diagnosis_code5#60932, icd_diagnosis_code6#60933, icd_diagnosis_code7#60934, icd_diagnosis_code8#60935, icd_diagnosis_code9#60936, principal_procedure#60937, procedure_code2#60938, procedure_code3#60939, procedure_code4#60940, procedure_code5#60941, procedure_code6#60942, cpt_modifier#60943, ... 53 more fields]\n",
       "   :  +- Relation [procedure_code#60920,DRG#60921,fachdid#60922,member_id#60923,claim_number#60924,seqnum#60925,marketscan_version_number#60926,principal_diagnosis#60927,icd_diagnosis_code1#60928,icd_diagnosis_code2#60929,icd_diagnosis_code3#60930,icd_diagnosis_code4#60931,icd_diagnosis_code5#60932,icd_diagnosis_code6#60933,icd_diagnosis_code7#60934,icd_diagnosis_code8#60935,icd_diagnosis_code9#60936,principal_procedure#60937,procedure_code2#60938,procedure_code3#60939,procedure_code4#60940,procedure_code5#60941,procedure_code6#60942,cpt_modifier#60943,... 51 more fields] parquet\n",
       "   +- Project [procedure_code#61070, DRG#61071, fachdid#61072, member_id#61073, claim_number#61074, seqnum#61075, marketscan_version_number#61076, principal_diagnosis#61077, icd_diagnosis_code1#61078, icd_diagnosis_code2#61079, icd_diagnosis_code3#61080, icd_diagnosis_code4#61081, icd_diagnosis_code5#61082, icd_diagnosis_code6#61083, icd_diagnosis_code7#61084, icd_diagnosis_code8#61085, icd_diagnosis_code9#61086, principal_procedure#61087, procedure_code2#61088, procedure_code3#61089, procedure_code4#61090, procedure_code5#61091, procedure_code6#61092, cpt_modifier#61093, ... 53 more fields]\n",
       "      +- Relation [procedure_code#61070,DRG#61071,fachdid#61072,member_id#61073,claim_number#61074,seqnum#61075,marketscan_version_number#61076,principal_diagnosis#61077,icd_diagnosis_code1#61078,icd_diagnosis_code2#61079,icd_diagnosis_code3#61080,icd_diagnosis_code4#61081,icd_diagnosis_code5#61082,icd_diagnosis_code6#61083,icd_diagnosis_code7#61084,icd_diagnosis_code8#61085,icd_diagnosis_code9#61086,principal_procedure#61087,procedure_code2#61088,procedure_code3#61089,procedure_code4#61090,procedure_code5#61091,procedure_code6#61092,cpt_modifier#61093,... 48 more fields] parquet\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `family_id` cannot be resolved. Did you mean one of the following? [`allowed`, `fachdid`, `member_id`, `prov_id`, `admit_type`]. SQLSTATE: 42703;\n'Project [procedure_code#60768, DRG#60769, fachdid#60770, member_id#60771, claim_number#60772, seqnum#60773, marketscan_version_number#60774, 'family_id, principal_diagnosis#60775, icd_diagnosis_code1#60776, icd_diagnosis_code2#60777, icd_diagnosis_code3#60778, icd_diagnosis_code4#60779, icd_diagnosis_code5#60780, icd_diagnosis_code6#60781, icd_diagnosis_code7#60782, icd_diagnosis_code8#60783, icd_diagnosis_code9#60784, principal_procedure#60785, procedure_code2#60786, procedure_code3#60787, procedure_code4#60788, procedure_code5#60789, procedure_code6#60790, ... 65 more fields]\n+- Union false, false\n   :- Project [procedure_code#60768, DRG#60769, fachdid#60770, member_id#60771, claim_number#60772, seqnum#60773, marketscan_version_number#60774, principal_diagnosis#60775, icd_diagnosis_code1#60776, icd_diagnosis_code2#60777, icd_diagnosis_code3#60778, icd_diagnosis_code4#60779, icd_diagnosis_code5#60780, icd_diagnosis_code6#60781, icd_diagnosis_code7#60782, icd_diagnosis_code8#60783, icd_diagnosis_code9#60784, principal_procedure#60785, procedure_code2#60786, procedure_code3#60787, procedure_code4#60788, procedure_code5#60789, procedure_code6#60790, cpt_modifier#60791, ... 53 more fields]\n   :  +- Relation [procedure_code#60768,DRG#60769,fachdid#60770,member_id#60771,claim_number#60772,seqnum#60773,marketscan_version_number#60774,principal_diagnosis#60775,icd_diagnosis_code1#60776,icd_diagnosis_code2#60777,icd_diagnosis_code3#60778,icd_diagnosis_code4#60779,icd_diagnosis_code5#60780,icd_diagnosis_code6#60781,icd_diagnosis_code7#60782,icd_diagnosis_code8#60783,icd_diagnosis_code9#60784,principal_procedure#60785,procedure_code2#60786,procedure_code3#60787,procedure_code4#60788,procedure_code5#60789,procedure_code6#60790,cpt_modifier#60791,... 52 more fields] parquet\n   :- Project [procedure_code#60920, DRG#60921, fachdid#60922, member_id#60923, claim_number#60924, seqnum#60925, marketscan_version_number#60926, principal_diagnosis#60927, icd_diagnosis_code1#60928, icd_diagnosis_code2#60929, icd_diagnosis_code3#60930, icd_diagnosis_code4#60931, icd_diagnosis_code5#60932, icd_diagnosis_code6#60933, icd_diagnosis_code7#60934, icd_diagnosis_code8#60935, icd_diagnosis_code9#60936, principal_procedure#60937, procedure_code2#60938, procedure_code3#60939, procedure_code4#60940, procedure_code5#60941, procedure_code6#60942, cpt_modifier#60943, ... 53 more fields]\n   :  +- Relation [procedure_code#60920,DRG#60921,fachdid#60922,member_id#60923,claim_number#60924,seqnum#60925,marketscan_version_number#60926,principal_diagnosis#60927,icd_diagnosis_code1#60928,icd_diagnosis_code2#60929,icd_diagnosis_code3#60930,icd_diagnosis_code4#60931,icd_diagnosis_code5#60932,icd_diagnosis_code6#60933,icd_diagnosis_code7#60934,icd_diagnosis_code8#60935,icd_diagnosis_code9#60936,principal_procedure#60937,procedure_code2#60938,procedure_code3#60939,procedure_code4#60940,procedure_code5#60941,procedure_code6#60942,cpt_modifier#60943,... 51 more fields] parquet\n   +- Project [procedure_code#61070, DRG#61071, fachdid#61072, member_id#61073, claim_number#61074, seqnum#61075, marketscan_version_number#61076, principal_diagnosis#61077, icd_diagnosis_code1#61078, icd_diagnosis_code2#61079, icd_diagnosis_code3#61080, icd_diagnosis_code4#61081, icd_diagnosis_code5#61082, icd_diagnosis_code6#61083, icd_diagnosis_code7#61084, icd_diagnosis_code8#61085, icd_diagnosis_code9#61086, principal_procedure#61087, procedure_code2#61088, procedure_code3#61089, procedure_code4#61090, procedure_code5#61091, procedure_code6#61092, cpt_modifier#61093, ... 53 more fields]\n      +- Relation [procedure_code#61070,DRG#61071,fachdid#61072,member_id#61073,claim_number#61074,seqnum#61075,marketscan_version_number#61076,principal_diagnosis#61077,icd_diagnosis_code1#61078,icd_diagnosis_code2#61079,icd_diagnosis_code3#61080,icd_diagnosis_code4#61081,icd_diagnosis_code5#61082,icd_diagnosis_code6#61083,icd_diagnosis_code7#61084,icd_diagnosis_code8#61085,icd_diagnosis_code9#61086,principal_procedure#61087,procedure_code2#61088,procedure_code3#61089,procedure_code4#61090,procedure_code5#61091,procedure_code6#61092,cpt_modifier#61093,... 48 more fields] parquet\n"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "UNRESOLVED_COLUMN.WITH_SUGGESTION",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": "42703",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-3451601888346498>, line 121\u001B[0m\n\u001B[1;32m     28\u001B[0m column_order \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprocedure_code\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     29\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDRG\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     30\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfachdid\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYEAR\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    116\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMS_Source_File\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    120\u001B[0m \u001B[38;5;66;03m# Reorder the columns in Combined_table\u001B[39;00m\n\u001B[0;32m--> 121\u001B[0m Combined_table \u001B[38;5;241m=\u001B[39m Combined_table\u001B[38;5;241m.\u001B[39mselect([col(c) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m column_order])\n\u001B[1;32m    125\u001B[0m target_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medclaims_combined.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    127\u001B[0m Combined_table\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYEAR\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMS_Source_File\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mparquet(target_path, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3853\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   3808\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   3809\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   3810\u001B[0m \n\u001B[1;32m   3811\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3851\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n\u001B[1;32m   3852\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3853\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jcols(\u001B[38;5;241m*\u001B[39mcols))\n\u001B[1;32m   3854\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `family_id` cannot be resolved. Did you mean one of the following? [`allowed`, `fachdid`, `member_id`, `prov_id`, `admit_type`]. SQLSTATE: 42703;\n'Project [procedure_code#60768, DRG#60769, fachdid#60770, member_id#60771, claim_number#60772, seqnum#60773, marketscan_version_number#60774, 'family_id, principal_diagnosis#60775, icd_diagnosis_code1#60776, icd_diagnosis_code2#60777, icd_diagnosis_code3#60778, icd_diagnosis_code4#60779, icd_diagnosis_code5#60780, icd_diagnosis_code6#60781, icd_diagnosis_code7#60782, icd_diagnosis_code8#60783, icd_diagnosis_code9#60784, principal_procedure#60785, procedure_code2#60786, procedure_code3#60787, procedure_code4#60788, procedure_code5#60789, procedure_code6#60790, ... 65 more fields]\n+- Union false, false\n   :- Project [procedure_code#60768, DRG#60769, fachdid#60770, member_id#60771, claim_number#60772, seqnum#60773, marketscan_version_number#60774, principal_diagnosis#60775, icd_diagnosis_code1#60776, icd_diagnosis_code2#60777, icd_diagnosis_code3#60778, icd_diagnosis_code4#60779, icd_diagnosis_code5#60780, icd_diagnosis_code6#60781, icd_diagnosis_code7#60782, icd_diagnosis_code8#60783, icd_diagnosis_code9#60784, principal_procedure#60785, procedure_code2#60786, procedure_code3#60787, procedure_code4#60788, procedure_code5#60789, procedure_code6#60790, cpt_modifier#60791, ... 53 more fields]\n   :  +- Relation [procedure_code#60768,DRG#60769,fachdid#60770,member_id#60771,claim_number#60772,seqnum#60773,marketscan_version_number#60774,principal_diagnosis#60775,icd_diagnosis_code1#60776,icd_diagnosis_code2#60777,icd_diagnosis_code3#60778,icd_diagnosis_code4#60779,icd_diagnosis_code5#60780,icd_diagnosis_code6#60781,icd_diagnosis_code7#60782,icd_diagnosis_code8#60783,icd_diagnosis_code9#60784,principal_procedure#60785,procedure_code2#60786,procedure_code3#60787,procedure_code4#60788,procedure_code5#60789,procedure_code6#60790,cpt_modifier#60791,... 52 more fields] parquet\n   :- Project [procedure_code#60920, DRG#60921, fachdid#60922, member_id#60923, claim_number#60924, seqnum#60925, marketscan_version_number#60926, principal_diagnosis#60927, icd_diagnosis_code1#60928, icd_diagnosis_code2#60929, icd_diagnosis_code3#60930, icd_diagnosis_code4#60931, icd_diagnosis_code5#60932, icd_diagnosis_code6#60933, icd_diagnosis_code7#60934, icd_diagnosis_code8#60935, icd_diagnosis_code9#60936, principal_procedure#60937, procedure_code2#60938, procedure_code3#60939, procedure_code4#60940, procedure_code5#60941, procedure_code6#60942, cpt_modifier#60943, ... 53 more fields]\n   :  +- Relation [procedure_code#60920,DRG#60921,fachdid#60922,member_id#60923,claim_number#60924,seqnum#60925,marketscan_version_number#60926,principal_diagnosis#60927,icd_diagnosis_code1#60928,icd_diagnosis_code2#60929,icd_diagnosis_code3#60930,icd_diagnosis_code4#60931,icd_diagnosis_code5#60932,icd_diagnosis_code6#60933,icd_diagnosis_code7#60934,icd_diagnosis_code8#60935,icd_diagnosis_code9#60936,principal_procedure#60937,procedure_code2#60938,procedure_code3#60939,procedure_code4#60940,procedure_code5#60941,procedure_code6#60942,cpt_modifier#60943,... 51 more fields] parquet\n   +- Project [procedure_code#61070, DRG#61071, fachdid#61072, member_id#61073, claim_number#61074, seqnum#61075, marketscan_version_number#61076, principal_diagnosis#61077, icd_diagnosis_code1#61078, icd_diagnosis_code2#61079, icd_diagnosis_code3#61080, icd_diagnosis_code4#61081, icd_diagnosis_code5#61082, icd_diagnosis_code6#61083, icd_diagnosis_code7#61084, icd_diagnosis_code8#61085, icd_diagnosis_code9#61086, principal_procedure#61087, procedure_code2#61088, procedure_code3#61089, procedure_code4#61090, procedure_code5#61091, procedure_code6#61092, cpt_modifier#61093, ... 53 more fields]\n      +- Relation [procedure_code#61070,DRG#61071,fachdid#61072,member_id#61073,claim_number#61074,seqnum#61075,marketscan_version_number#61076,principal_diagnosis#61077,icd_diagnosis_code1#61078,icd_diagnosis_code2#61079,icd_diagnosis_code3#61080,icd_diagnosis_code4#61081,icd_diagnosis_code5#61082,icd_diagnosis_code6#61083,icd_diagnosis_code7#61084,icd_diagnosis_code8#61085,icd_diagnosis_code9#61086,principal_procedure#61087,procedure_code2#61088,procedure_code3#61089,procedure_code4#61090,procedure_code5#61091,procedure_code6#61092,cpt_modifier#61093,... 48 more fields] parquet\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\t\t\t\t\t\t\t\t\n",
    "/*\n",
    "DROP VIEW IF EXISTS LDS_view;\t\t\t\t\t\t\t\t\n",
    "CREATE TEMPORARY VIEW LDS_view \t\t\t\t\t\t\t\t\n",
    "USING PARQUET\t\t\t\t\t\t\t\t\n",
    "OPTIONS (\t\t\t\t\t\t\t\t\n",
    " \t\t\t\t\t\t\t\t\n",
    " path \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medciare_medclaims_combined.parquet\"\n",
    " \t\t\t\t\t\t\t\t\n",
    " );\t\t\t\t\t\t\t\t\n",
    "\n",
    "-- \"wasbs://marketscan@rgdevglobalreahl.blob.core.windows.net/marketscan/medicaid/marketscan-preprod-step2\"\n",
    "-- \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/medicaid/medical/ms_medicaid_medclaims_combined.parquet\"\n",
    "\n",
    "select member_id,YEAR,\n",
    "sum(allowed) as allowed\n",
    "from LDS_view\n",
    "group by 1,2\n",
    "order by 3 asc\n",
    "\n",
    "*/\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3879550410129494,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Service_Category_Logic_MS_Medicaid",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}