{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f44da14-36fd-4ee1-9403-f3c53bf647a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n##### Write to parquet\\ntarget_path = \"wasbs://marketscan@rgdevglobalreahl.blob.core.windows.net/marketscan/commercial/marketscan-preprod-step2\"\\njoined_df.write.partitionBy(\"YEAR\", \"MS_Source_File\").parquet(target_path, mode=\"overwrite\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%python \n",
    "\n",
    "#### Use this code to configure your accoutn before running SQL in Databricks\n",
    "\n",
    "storage_account_name = 'rgdevglobalreahl'\n",
    "storage_account_key =  'RjnFSNg2IDzQ5bN/8aTZOFK1lnpL6zytdMnuQIrJQMw6psa7++18fCoIfaLkq5DEFV3Hp0b7WZjR+AStQuhR+A=='\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col, lit, round \n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import expr, md5, concat_ws, coalesce, when\n",
    "\n",
    "'''\n",
    "##### Write to parquet\n",
    "target_path = \"wasbs://marketscan@rgdevglobalreahl.blob.core.windows.net/marketscan/commercial/marketscan-preprod-step2\"\n",
    "joined_df.write.partitionBy(\"YEAR\", \"MS_Source_File\").parquet(target_path, mode=\"overwrite\")\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e92e246-fed6-4808-b396-69f538a384a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####### Set Claims Data\n",
    "claims_source = \"wasbs://marketscan@rgdevglobalreahl.blob.core.windows.net/marketscan/commercial/marketscan-preprod-step2\"\n",
    "df = spark.read.parquet(claims_source)\n",
    "\n",
    "\n",
    "### TOB mapping\n",
    "ip_tob_prefixes = [\"11\", \"12\", \"17\", \"21\", \"22\", \"81\", \"82\", \"84\", \"85\", \"86\"]\n",
    "snf_tob_prefixes = [\"17\", \"21\", \"22\"]\n",
    "hospice_tob_prefixes = [\"81\", \"82\", \"65\"]\n",
    "\n",
    "#### DRG Mapping (Upload after revising yearly)\n",
    "drg_mapping = pd.read_csv('DRG_Mapping.csv', dtype={\"DRG\": str})\n",
    "drg_map_spark = spark.createDataFrame(drg_mapping)\n",
    "drg_map_spark = drg_map_spark.withColumn(\"DRG\", col(\"DRG\").cast(\"string\"))\n",
    "\n",
    "#### BETOS Mapping (Upload after revising yearly) - for Prof Mapping HCPCS & CPT Codes\n",
    "betos_mapping = pd.read_csv('CMS_BETOS.csv', dtype={\"procedure_code\": str})\n",
    "betos_mapping_spark = spark.createDataFrame(betos_mapping)\n",
    "betos_mapping_spark = betos_mapping_spark.withColumn(\"procedure_code\", col(\"procedure_code\").cast(\"string\"))\n",
    "\n",
    "\n",
    "#########__________________Major Service Cat_____________________#####################\n",
    "\n",
    "\n",
    "### Major Service Category\n",
    "df = df.withColumn(\"major_service_category\", F.when((col(\"DRG\").isNull() & col(\"bill_type\").isNull()), \"Professional\").otherwise(\n",
    "    F.when(col(\"DRG\").isNotNull(), \"Inpatient Facility\").otherwise(F.when(F.col(\"bill_type\").substr(1, 2).isin(ip_tob_prefixes), \"Inpatient Facility\").otherwise(\"Outpatient Facility\"))))\n",
    "\n",
    "\n",
    "#########__________________IP Detailed Service Cat_____________________#####################\n",
    "#### Detailed Service Categories\n",
    "### IP Step 1: Hospice & SNF based on TOB\n",
    "df = df.withColumn(\"detailed_service_category_ip_HospSNF\", \n",
    "                   F.when((col(\"major_service_category\") == \"Inpatient Facility\") & (col(\"bill_type\").substr(1,2).isin(snf_tob_prefixes)), \"SNF\").otherwise(\n",
    "                       F.when((col(\"major_service_category\") == \"Inpatient Facility\") & (col(\"bill_type\").substr(1,2).isin(hospice_tob_prefixes)), \"Hospice\").otherwise(\n",
    "                        None)))\n",
    "\n",
    "### IP Step 2: Join with mapped DRGS\n",
    "df = df.join(\n",
    "    drg_map_spark.select(\"DRG\",\"Level_II\", \"Level_III\"),\n",
    "    on=\"DRG\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Use DRG mapping or previously determined categories\n",
    "df = df.withColumn(\"IP_level_II\", F.when(col(\"major_service_category\") == \"Inpatient Facility\", F.coalesce( col(\"detailed_service_category_ip_HospSNF\"), col(\"Level_II\"), lit(\"Other\"))).otherwise(None))\n",
    "df = df.withColumn(\"IP_level_III\", F.when(col(\"major_service_category\") == \"Inpatient Facility\", F.coalesce(col(\"detailed_service_category_ip_HospSNF\"), col(\"Level_III\"), lit(\"Other Inpatient Facility\"))).otherwise(None))\n",
    "\n",
    "df = df.drop(\"detailed_service_category_ip_HospSNF\", \"Level_II\", \"Level_III\")\n",
    "\n",
    "\n",
    "##########Note ... redo level II as level IV - more detail and map acute IP to level II ###########\n",
    "\n",
    "\n",
    "#########__________________Prof Detailed Service Cat_____________________#####################\n",
    "### Prof Step 1: Join the BETOS Mapping\n",
    "df = df.join(\n",
    "    betos_mapping_spark.select(\"procedure_code\",\"RBCS_Cat_Desc\", \"RBCS_SubCat_Desc\"),\n",
    "    on=\"procedure_code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "### Prof Step 2: Null out everything prof for level II and level III\n",
    "df = df.withColumn(\"Prof_level_II\", F.when(col(\"major_service_category\") == \"Professional\", F.coalesce(col(\"RBCS_Cat_Desc\"), lit(\"Other Professional\"))).otherwise(None))\n",
    "df = df.withColumn(\"Prof_level_III\", F.when(col(\"major_service_category\") == \"Professional\", F.coalesce(col(\"RBCS_SubCat_Desc\"), lit(\"Other Professional\"))).otherwise(None))\n",
    "\n",
    "df = df.drop(\"RBCS_Cat_Desc\", \"RBCS_SubCat_Desc\")\n",
    "\n",
    "#########__________________OP Detailed Service Cat_____________________#####################\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"OP_Level_III\",\n",
    "    expr(\"\"\"\n",
    "        CASE\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND bill_type LIKE '3%' THEN 'Home Health'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '037%' THEN 'Anesthesia'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '054%' THEN 'Ambulance'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '090%' THEN 'Behavioral Health'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '091%' THEN 'Behavioral Health'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '100%' THEN 'Behavioral Health'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '080%' THEN 'Dialysis'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '082%' THEN 'Dialysis'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '083%' THEN 'Dialysis'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '084%' THEN 'Dialysis'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '085%' THEN 'Dialysis'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '088%' THEN 'Dialysis'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '073%' THEN 'EKG/ECG/EEG'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '074%' THEN 'EKG/ECG/EEG'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '045%' THEN 'Emergency Room'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '025%' THEN 'Facility Dispensed Pharmacy'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '063%' THEN 'Facility Dispensed Pharmacy'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '089%' THEN 'Facility Dispensed Pharmacy'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '030%' THEN 'Lab/Pathology'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '031%' THEN 'Facility Dispensed Pharmacy'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '076%' THEN 'Observation'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '042%' THEN 'PT/OT/ST'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '043%' THEN 'PT/OT/ST'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '044%' THEN 'PT/OT/ST'\n",
    "\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '051%' THEN 'Outpatient Clinic'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '052%' THEN 'Outpatient Clinic'\n",
    "        \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '026%' THEN 'Outpatient IV Therapy'\n",
    "\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code in('0360','0361','0362','0367','0369','0481','0490', '0499', '0790', '0799')  THEN 'Outpatient Surgery'\n",
    "            \n",
    "            -- WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '087%' THEN 'Cell/Gene Therapy'\n",
    "            \n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '032%' THEN 'Radiology - Diagnostic'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '033%' THEN 'Radiology - Therapeutic'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '035%' THEN 'Radiology - CT'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '061%' THEN 'Radiology - MRI/MRA/MRT'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '034%' THEN 'Radiology - Nuclear Medicine'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '040%' THEN 'Radiology - Other'\n",
    "\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '072%' THEN 'Labor and Delivery'\n",
    "\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '029%' THEN 'DME/Prosthetics/Supplies'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '027%' THEN 'DME/Prosthetics/Supplies'\n",
    "            WHEN major_service_category = 'Outpatient Facility' AND revenue_code LIKE '062%' THEN 'DME/Prosthetics/Supplies'\n",
    "\n",
    "            WHEN major_service_category = 'Outpatient Facility' THEN 'Other Outpatient Facility'\n",
    "            \n",
    "            ELSE NULL\n",
    "        END\n",
    "    \"\"\"))\n",
    "\n",
    "\n",
    "### V1 of detailed svc cat, will get recategorized later\n",
    "df = df.withColumn(\"detailed_service_category_temp\", expr(\"\"\"\n",
    "    case when major_service_category = 'Inpatient Facility' Then  IP_Level_III\n",
    "    when major_service_category = 'Outpatient Facility' Then  OP_Level_III\n",
    "    when major_service_category = 'Professional' Then  Prof_level_III\n",
    "    else NULL end\n",
    "    \"\"\"))\n",
    "\n",
    "\n",
    "### V1 of service cat ID, will get recategorized later\n",
    "df = df.withColumn(\n",
    "    \"hash_key_temp\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"detailed_service_category_temp\", \"Incurred_Date\"))\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "### Bill type 12X found to be re-billed as OP rules in an IP stay - this hierarchy ensures that the one billed with DRG comes first in the window function if there is a tie with incurred date\n",
    "df = df.withColumn(\"IP_DSC_Hierarchy\", expr(\"\"\"\n",
    "    case when IP_level_III = 'Other Inpatient Facility' Then  3\n",
    "    when IP_level_III in ('SNF', 'Hospice') Then  2\n",
    "    else 1 end\n",
    "    \"\"\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Write and Save\n",
    "target_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/commercial/medical/ms_commercial_medical.parquet\" \n",
    "df.write.partitionBy(\"YEAR\", \"MS_Source_File\").parquet(target_path, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "734e5ff7-1f54-4ace-9a73-9337d0a568a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#########__________________Relabeling IP Adjacent Admissions (IP Acute)_____________________#####################\n",
    "\n",
    "\n",
    "####### Set Claims Data, Filter to IP Acute\n",
    "claims_source = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/commercial/medical/ms_commercial_medical.parquet\" \n",
    "df = spark.read.parquet(claims_source)\n",
    "\n",
    "df = df.filter((col(\"major_service_category\") == \"Inpatient Facility\") & \n",
    "               (~col(\"IP_level_III\").isin([\"SNF\", \"Hospice\"])))\n",
    "\n",
    "\n",
    "### Inucrred date needs to be changed to admit date for SNF/Hospice\n",
    "# List of columns to group by\n",
    "\n",
    "#hash key temp is concat of mbr ID, detailed svc cat tmp, inc date\n",
    "\n",
    "\n",
    "dedup_df = df\n",
    "'''.select(\"member_id\", \"service_to_date\", \"incurred_date\", \"IP_level_II\", \"detailed_service_category_temp\",\"hash_key_temp\",\"major_service_category\", \"admit_date\" ,\"IP_DSC_Hierarchy\").distinct().orderBy(\"member_id\", \"incurred_date\")\n",
    "'''\n",
    "\n",
    "# Define window by member_id ordered by admit_date\n",
    "window_spec = Window.partitionBy(\"member_id\").orderBy(\"incurred_date\", \"IP_DSC_Hierarchy\", \"service_to_date\")\n",
    "\n",
    "# Add lagged discharge_date\n",
    "'''dedup_df = df.withColumn(\"prev_service_to_date\", F.lag(\"service_to_date\").over(window_spec)).withColumn(\"prev_IP_level_III\", F.lag(\"detailed_service_category_temp\").over(window_spec)).withColumn(\"prev_IP_level_II\", F.lag(\"IP_level_II\").over(window_spec)).withColumn(\"prev_hash_key\", F.lag(\"hash_key_temp\").over(window_spec))\n",
    "'''\n",
    "\n",
    "\n",
    "# Sort by incurred_date\n",
    "window_group = Window.partitionBy(\"member_id\").orderBy(\"incurred_date\",\"IP_DSC_Hierarchy\" )\n",
    "\n",
    "# Track running max of service_to_date\n",
    "## Becomes temp new discharge date & we 1st aggregate / regroup all svcs that fall under this\n",
    "dedup_df = dedup_df.withColumn(\"running_max_service_to\", F.max(\"service_to_date\").over(window_group))\n",
    "\n",
    "############### _________ handling overlapping IP _________________#####\n",
    "\n",
    "### create this key to handle overlapping IP svcs\n",
    "dedup_df = dedup_df.withColumn(\n",
    "    \"hash_key_temp_step2\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"running_max_service_to\"))\n",
    ")\n",
    "\n",
    "\n",
    "window_spec = Window.partitionBy(\"member_id\", \"hash_key_temp_step2\")\n",
    "\n",
    "### not used later??\n",
    "dedup_df = dedup_df.withColumn(\"min_incurred_date_over_hash_step2\", F.min(\"incurred_date\").over(window_spec))\n",
    "dedup_df = dedup_df.withColumn(\"min_IP_Hierarch_over_hash_step2\", F.min(\"IP_DSC_Hierarchy\").over(window_spec))\n",
    "\n",
    "\n",
    "\n",
    "#####___________________________\n",
    "\n",
    "#### do unique to handle multiple incurred dates and look at only one date at a time, not multiple ... for the purpose of then counting row numbers 1-1 with each encounter\n",
    "IP_svc_hash = dedup_df.select(\"hash_key_temp_step2\", \"member_id\", \"detailed_service_category_temp\", \"IP_level_II\", \"incurred_date\", \"IP_DSC_Hierarchy\").orderBy(\"incurred_date\", \"IP_DSC_Hierarchy\").distinct()\n",
    "\n",
    "\n",
    "### Keep on the 1st occuring value -- deduped\n",
    "window_spec = Window.partitionBy(\"hash_key_temp_step2\").orderBy(\"incurred_date\", \"IP_DSC_Hierarchy\")\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "IP_svc_hash = IP_svc_hash.filter(col(\"row_num\") == 1).distinct()\n",
    "\n",
    "\n",
    "dedup_df = dedup_df.join (\n",
    "    IP_svc_hash.select(\"hash_key_temp_step2\", \"detailed_service_category_temp\", \"IP_level_II\").withColumnRenamed(\"detailed_service_category_temp\", \"detailed_service_category_override\").withColumnRenamed(\"IP_level_II\", \"IP_level_II_override\"), \n",
    "    on=\"hash_key_temp_step2\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "### create new key to aggregate over - with revised svc categories after condensing overlapping dates by member\n",
    "dedup_df = dedup_df.withColumn(\n",
    "    \"svc_overide_hash\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"detailed_service_category_override\", \"min_incurred_date_over_hash_step2\")))\n",
    "\n",
    "\n",
    "############### _________ handling adjacent IP _________________#####\n",
    "\n",
    "dedup_df = dedup_df.withColumn(\n",
    "    \"hash_key_temp_step3\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"min_incurred_date_over_hash_step2\"))\n",
    ")\n",
    "\n",
    "IP_svc_hash = dedup_df.select(\"member_id\", \"running_max_service_to\", \"min_incurred_date_over_hash_step2\",\"hash_key_temp_step3\", \"detailed_service_category_override\", \"IP_level_II_override\", \"svc_overide_hash\" ).orderBy(\"incurred_date\").distinct()\n",
    "\n",
    "\n",
    "# Sort by Start_date\n",
    "window_spec = Window.partitionBy(\"member_id\").orderBy(\"min_incurred_date_over_hash_step2\")\n",
    "\n",
    "# Get previous end_date\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"prev_end\", F.lag(\"running_max_service_to\").over(window_spec))\n",
    "\n",
    "# Flag new episode when not overlapping or adjacent (i.e., Start_date > prev_end + 1)\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\n",
    "    \"new_episode_flag\",\n",
    "    F.when(\n",
    "        (F.col(\"prev_end\").isNull()) |\n",
    "        (F.datediff(\"min_incurred_date_over_hash_step2\", \"prev_end\") >= 1),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# Assign episode ID using cumulative sum\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"episode_id\", F.sum(\"new_episode_flag\").over(window_spec))\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"episode_id_hash\",  md5(concat_ws(\"||\", \"member_id\", \"episode_id\")))\n",
    "\n",
    "\n",
    "\n",
    "####__________________\n",
    "window_spec = Window.partitionBy(\"episode_id_hash\").orderBy(\"min_incurred_date_over_hash_step2\")\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"row_num_adjacent_IP\", F.row_number().over(window_spec))\n",
    "\n",
    "IP_svc_hash_dedup = IP_svc_hash.filter(col(\"row_num_adjacent_IP\") == 1).distinct()\n",
    "\n",
    "IP_svc_hash_dedup = IP_svc_hash_dedup.select(\"member_id\", \"detailed_service_category_override\", \"IP_level_II_override\",\"episode_id_hash\", \"svc_overide_hash\")\n",
    "\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.join (\n",
    "    IP_svc_hash_dedup.select( \"episode_id_hash\", \"detailed_service_category_override\", \"IP_level_II_override\").withColumnRenamed(\"detailed_service_category_override\", \"detailed_service_category_override_final\").withColumnRenamed(\"IP_level_II_override\", \"IP_level_II_override_final\"), \n",
    "    on=\"episode_id_hash\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "### Calc final admit / discharge\n",
    "window_spec = Window.partitionBy(\"episode_id_hash\")\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"final_IP_admit\", F.min(\"min_incurred_date_over_hash_step2\").over(window_spec))\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"final_IP_discharge\", F.max(\"running_max_service_to\").over(window_spec))\n",
    "\n",
    "#### join back to full claims\n",
    "#### LEFT OFF CHECK HERE --- svc hash formbr vs. claims\n",
    "\n",
    "\n",
    "dedup_df = dedup_df.join(\n",
    "    IP_svc_hash.select(\"svc_overide_hash\", \"detailed_service_category_override_final\", \"IP_level_II_override_final\", \"final_IP_admit\", \"final_IP_discharge\"),\n",
    "    on=\"svc_overide_hash\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "### drop extra columns\n",
    "\n",
    "dedup_df = dedup_df.drop(\"hash_key_temp_step3\", \"IP_level_II_override\", \"detailed_service_category_override\", \"min_IP_Hierarch_over_hash_step2\", \"min_incurred_date_over_hash_step2\", \"running_max_service_to\", \"hash_key_temp_step2\",\"svc_overide_hash\", \"detailed_service_category_temp\", \"hash_key_temp\", \"detailed_service_category_temp\thash_key_temp\tIP_DSC_Hierarchy\")\n",
    "\n",
    "\n",
    "## clean up steps\n",
    "### there is a very small amount of rows that sometime get categorized as NULL -- not sure why; but it was ~ < 0.1% of IP acute medical spend after runnning a number of times. \n",
    "dedup_df = (\n",
    "    dedup_df\n",
    "    .withColumn(\n",
    "        \"detailed_service_category_override_final\",\n",
    "        coalesce(col(\"detailed_service_category_override_final\"), lit(\"Medical\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"IP_level_II_override_final\",\n",
    "        coalesce(col(\"IP_level_II_override_final\"), lit(\"Ungrouped\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"final_IP_admit\",\n",
    "        coalesce(col(\"final_IP_admit\"), col(\"incurred_date\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"final_IP_discharge\",\n",
    "        coalesce(col(\"final_IP_discharge\"), col(\"service_to_date\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"IP_detailed_svccat_errorflag\",\n",
    "        when(col(\"detailed_service_category_override_final\").isNull(), lit(True)).otherwise(lit(False))\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "target_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/commercial/medical/ms_comm_medclaims_IPAcute.parquet\" \n",
    "dedup_df.write.parquet(target_path, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8daf5c6-ba56-46c5-9af4-e244dfdede81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#########__________________Relabeling IP Adjacent Admissions (SNF)_____________________#####################\n",
    "\n",
    "\n",
    "####### Set Claims Data, Filter to IP Acute\n",
    "claims_source = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/commercial/medical/ms_commercial_medical.parquet\" \n",
    "df = spark.read.parquet(claims_source)\n",
    "\n",
    "df = df.filter((col(\"major_service_category\") == \"Inpatient Facility\") & \n",
    "               (col(\"IP_level_III\").isin([\"SNF\"])))\n",
    "\n",
    "\n",
    "### Inucrred date needs to be changed to admit date for SNF/Hospice\n",
    "# List of columns to group by\n",
    "\n",
    "#hash key temp is concat of mbr ID, detailed svc cat tmp, inc date\n",
    "\n",
    "\n",
    "dedup_df = df\n",
    "'''.select(\"member_id\", \"service_to_date\", \"incurred_date\", \"IP_level_II\", \"detailed_service_category_temp\",\"hash_key_temp\",\"major_service_category\", \"admit_date\" ,\"IP_DSC_Hierarchy\").distinct().orderBy(\"member_id\", \"incurred_date\")\n",
    "'''\n",
    "\n",
    "# Define window by member_id ordered by admit_date\n",
    "window_spec = Window.partitionBy(\"member_id\").orderBy(\"incurred_date\", \"IP_DSC_Hierarchy\", \"service_to_date\")\n",
    "\n",
    "# Add lagged discharge_date\n",
    "'''dedup_df = df.withColumn(\"prev_service_to_date\", F.lag(\"service_to_date\").over(window_spec)).withColumn(\"prev_IP_level_III\", F.lag(\"detailed_service_category_temp\").over(window_spec)).withColumn(\"prev_IP_level_II\", F.lag(\"IP_level_II\").over(window_spec)).withColumn(\"prev_hash_key\", F.lag(\"hash_key_temp\").over(window_spec))\n",
    "'''\n",
    "\n",
    "\n",
    "# Sort by incurred_date\n",
    "window_group = Window.partitionBy(\"member_id\").orderBy(\"incurred_date\",\"IP_DSC_Hierarchy\" )\n",
    "\n",
    "# Track running max of service_to_date\n",
    "dedup_df = dedup_df.withColumn(\"running_max_service_to\", F.max(\"service_to_date\").over(window_group))\n",
    "\n",
    "############### _________ handling overlapping IP _________________#####\n",
    "dedup_df = dedup_df.withColumn(\n",
    "    \"hash_key_temp_step2\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"running_max_service_to\"))\n",
    ")\n",
    "\n",
    "\n",
    "window_spec = Window.partitionBy(\"member_id\", \"hash_key_temp_step2\")\n",
    "\n",
    "dedup_df = dedup_df.withColumn(\"min_incurred_date_over_hash_step2\", F.min(\"incurred_date\").over(window_spec))\n",
    "\n",
    "dedup_df = dedup_df.withColumn(\"min_IP_Hierarch_over_hash_step2\", F.min(\"IP_DSC_Hierarchy\").over(window_spec))\n",
    "\n",
    "\n",
    "\n",
    "#####___________________________\n",
    "\n",
    "IP_svc_hash = dedup_df.select(\"hash_key_temp_step2\", \"member_id\", \"detailed_service_category_temp\", \"IP_level_II\", \"incurred_date\", \"IP_DSC_Hierarchy\").orderBy(\"incurred_date\", \"IP_DSC_Hierarchy\").distinct()\n",
    "\n",
    "window_spec = Window.partitionBy(\"hash_key_temp_step2\").orderBy(\"incurred_date\", \"IP_DSC_Hierarchy\")\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.filter(col(\"row_num\") == 1).distinct()\n",
    "\n",
    "\n",
    "dedup_df = dedup_df.join (\n",
    "    IP_svc_hash.select(\"hash_key_temp_step2\", \"detailed_service_category_temp\", \"IP_level_II\").withColumnRenamed(\"detailed_service_category_temp\", \"detailed_service_category_override\").withColumnRenamed(\"IP_level_II\", \"IP_level_II_override\"), \n",
    "    on=\"hash_key_temp_step2\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "dedup_df = dedup_df.withColumn(\n",
    "    \"svc_overide_hash\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"detailed_service_category_override\", \"min_incurred_date_over_hash_step2\")))\n",
    "\n",
    "\n",
    "############### _________ handling adjacent IP _________________#####\n",
    "\n",
    "dedup_df = dedup_df.withColumn(\n",
    "    \"hash_key_temp_step3\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"min_incurred_date_over_hash_step2\"))\n",
    ")\n",
    "\n",
    "IP_svc_hash = dedup_df.select(\"member_id\", \"running_max_service_to\", \"min_incurred_date_over_hash_step2\",\"hash_key_temp_step3\", \"detailed_service_category_override\", \"IP_level_II_override\", \"svc_overide_hash\" ).orderBy(\"incurred_date\").distinct()\n",
    "\n",
    "\n",
    "# Sort by Start_date\n",
    "window_spec = Window.partitionBy(\"member_id\").orderBy(\"min_incurred_date_over_hash_step2\")\n",
    "\n",
    "# Get previous end_date\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"prev_end\", F.lag(\"running_max_service_to\").over(window_spec))\n",
    "\n",
    "# Flag new episode when not overlapping or adjacent (i.e., Start_date > prev_end + 1)\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\n",
    "    \"new_episode_flag\",\n",
    "    F.when(\n",
    "        (F.col(\"prev_end\").isNull()) |\n",
    "        (F.datediff(\"min_incurred_date_over_hash_step2\", \"prev_end\") >= 1),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# Assign episode ID using cumulative sum\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"episode_id\", F.sum(\"new_episode_flag\").over(window_spec))\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"episode_id_hash\",  md5(concat_ws(\"||\", \"member_id\", \"episode_id\")))\n",
    "\n",
    "\n",
    "####__________________\n",
    "window_spec = Window.partitionBy(\"episode_id_hash\").orderBy(\"min_incurred_date_over_hash_step2\")\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"row_num_adjacent_IP\", F.row_number().over(window_spec))\n",
    "\n",
    "IP_svc_hash_dedup = IP_svc_hash.filter(col(\"row_num_adjacent_IP\") == 1).distinct()\n",
    "IP_svc_hash_dedup = IP_svc_hash_dedup.select(\"member_id\", \"detailed_service_category_override\", \"IP_level_II_override\",\"episode_id_hash\", \"svc_overide_hash\")\n",
    "\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.join (\n",
    "    IP_svc_hash_dedup.select( \"episode_id_hash\", \"detailed_service_category_override\", \"IP_level_II_override\").withColumnRenamed(\"detailed_service_category_override\", \"detailed_service_category_override_final\").withColumnRenamed(\"IP_level_II_override\", \"IP_level_II_override_final\"), \n",
    "    on=\"episode_id_hash\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "### Calc final admit / discharge\n",
    "window_spec = Window.partitionBy(\"episode_id_hash\")\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"final_IP_admit\", F.min(\"min_incurred_date_over_hash_step2\").over(window_spec))\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"final_IP_discharge\", F.max(\"running_max_service_to\").over(window_spec))\n",
    "\n",
    "dedup_df = dedup_df.join(\n",
    "    IP_svc_hash.select(\"svc_overide_hash\", \"detailed_service_category_override_final\", \"IP_level_II_override_final\", \"final_IP_admit\", \"final_IP_discharge\"),\n",
    "    on=\"svc_overide_hash\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "### drop extra columns\n",
    "dedup_df = dedup_df.drop(\"hash_key_temp_step3\", \"IP_level_II_override\", \"detailed_service_category_override\", \"min_IP_Hierarch_over_hash_step2\", \"min_incurred_date_over_hash_step2\", \"running_max_service_to\", \"hash_key_temp_step2\",\"svc_overide_hash\", \"detailed_service_category_temp\", \"hash_key_temp\", \"detailed_service_category_temp\thash_key_temp\tIP_DSC_Hierarchy\")\n",
    "\n",
    "\n",
    "## clean up steps\n",
    "### there is a very small amount of rows that sometime get categorized as NULL -- not sure why; but it was ~ < 0.1% of IP acute medical spend after runnning a number of times. \n",
    "dedup_df = (\n",
    "    dedup_df\n",
    "    .withColumn(\n",
    "        \"detailed_service_category_override_final\",\n",
    "        coalesce(col(\"detailed_service_category_override_final\"), lit(\"Medical\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"IP_level_II_override_final\",\n",
    "        coalesce(col(\"IP_level_II_override_final\"), lit(\"Ungrouped\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"final_IP_admit\",\n",
    "        coalesce(col(\"final_IP_admit\"), col(\"incurred_date\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"final_IP_discharge\",\n",
    "        coalesce(col(\"final_IP_discharge\"), col(\"service_to_date\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"IP_detailed_svccat_errorflag\",\n",
    "        when(col(\"detailed_service_category_override_final\").isNull(), lit(True)).otherwise(lit(False))\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "target_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/commercial/medical/ms_comm_medclaims_SNF.parquet\" \n",
    "dedup_df.write.parquet(target_path, mode=\"overwrite\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e20828b-d353-46b1-9811-8a0f2047f201",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#########__________________Relabeling IP Adjacent Admissions (Hospice)_____________________#####################\n",
    "\n",
    "\n",
    "####### Set Claims Data, Filter to IP Acute\n",
    "claims_source = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/commercial/medical/ms_commercial_medical.parquet\" \n",
    "df = spark.read.parquet(claims_source)\n",
    "\n",
    "df = df.filter((col(\"major_service_category\") == \"Inpatient Facility\") & \n",
    "               (col(\"IP_level_III\").isin([\"Hospice\"])))\n",
    "\n",
    "\n",
    "### Inucrred date needs to be changed to admit date for SNF/Hospice\n",
    "# List of columns to group by\n",
    "\n",
    "#hash key temp is concat of mbr ID, detailed svc cat tmp, inc date\n",
    "\n",
    "\n",
    "dedup_df = df\n",
    "'''.select(\"member_id\", \"service_to_date\", \"incurred_date\", \"IP_level_II\", \"detailed_service_category_temp\",\"hash_key_temp\",\"major_service_category\", \"admit_date\" ,\"IP_DSC_Hierarchy\").distinct().orderBy(\"member_id\", \"incurred_date\")\n",
    "'''\n",
    "\n",
    "# Define window by member_id ordered by admit_date\n",
    "window_spec = Window.partitionBy(\"member_id\").orderBy(\"incurred_date\", \"IP_DSC_Hierarchy\", \"service_to_date\")\n",
    "\n",
    "# Add lagged discharge_date\n",
    "'''dedup_df = df.withColumn(\"prev_service_to_date\", F.lag(\"service_to_date\").over(window_spec)).withColumn(\"prev_IP_level_III\", F.lag(\"detailed_service_category_temp\").over(window_spec)).withColumn(\"prev_IP_level_II\", F.lag(\"IP_level_II\").over(window_spec)).withColumn(\"prev_hash_key\", F.lag(\"hash_key_temp\").over(window_spec))\n",
    "'''\n",
    "\n",
    "\n",
    "# Sort by incurred_date\n",
    "window_group = Window.partitionBy(\"member_id\").orderBy(\"incurred_date\",\"IP_DSC_Hierarchy\" )\n",
    "\n",
    "# Track running max of service_to_date\n",
    "dedup_df = dedup_df.withColumn(\"running_max_service_to\", F.max(\"service_to_date\").over(window_group))\n",
    "\n",
    "############### _________ handling overlapping IP _________________#####\n",
    "dedup_df = dedup_df.withColumn(\n",
    "    \"hash_key_temp_step2\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"running_max_service_to\"))\n",
    ")\n",
    "\n",
    "\n",
    "window_spec = Window.partitionBy(\"member_id\", \"hash_key_temp_step2\")\n",
    "\n",
    "dedup_df = dedup_df.withColumn(\"min_incurred_date_over_hash_step2\", F.min(\"incurred_date\").over(window_spec))\n",
    "\n",
    "dedup_df = dedup_df.withColumn(\"min_IP_Hierarch_over_hash_step2\", F.min(\"IP_DSC_Hierarchy\").over(window_spec))\n",
    "\n",
    "\n",
    "\n",
    "#####___________________________\n",
    "\n",
    "IP_svc_hash = dedup_df.select(\"hash_key_temp_step2\", \"member_id\", \"detailed_service_category_temp\", \"IP_level_II\", \"incurred_date\", \"IP_DSC_Hierarchy\").orderBy(\"incurred_date\", \"IP_DSC_Hierarchy\").distinct()\n",
    "\n",
    "window_spec = Window.partitionBy(\"hash_key_temp_step2\").orderBy(\"incurred_date\", \"IP_DSC_Hierarchy\")\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.filter(col(\"row_num\") == 1).distinct()\n",
    "\n",
    "\n",
    "dedup_df = dedup_df.join (\n",
    "    IP_svc_hash.select(\"hash_key_temp_step2\", \"detailed_service_category_temp\", \"IP_level_II\").withColumnRenamed(\"detailed_service_category_temp\", \"detailed_service_category_override\").withColumnRenamed(\"IP_level_II\", \"IP_level_II_override\"), \n",
    "    on=\"hash_key_temp_step2\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "dedup_df = dedup_df.withColumn(\n",
    "    \"svc_overide_hash\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"detailed_service_category_override\", \"min_incurred_date_over_hash_step2\")))\n",
    "\n",
    "\n",
    "############### _________ handling adjacent IP _________________#####\n",
    "\n",
    "dedup_df = dedup_df.withColumn(\n",
    "    \"hash_key_temp_step3\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"min_incurred_date_over_hash_step2\"))\n",
    ")\n",
    "\n",
    "IP_svc_hash = dedup_df.select(\"member_id\", \"running_max_service_to\", \"min_incurred_date_over_hash_step2\",\"hash_key_temp_step3\", \"detailed_service_category_override\", \"IP_level_II_override\", \"svc_overide_hash\" ).orderBy(\"incurred_date\").distinct()\n",
    "\n",
    "\n",
    "# Sort by Start_date\n",
    "window_spec = Window.partitionBy(\"member_id\").orderBy(\"min_incurred_date_over_hash_step2\")\n",
    "\n",
    "# Get previous end_date\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"prev_end\", F.lag(\"running_max_service_to\").over(window_spec))\n",
    "\n",
    "# Flag new episode when not overlapping or adjacent (i.e., Start_date > prev_end + 1)\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\n",
    "    \"new_episode_flag\",\n",
    "    F.when(\n",
    "        (F.col(\"prev_end\").isNull()) |\n",
    "        (F.datediff(\"min_incurred_date_over_hash_step2\", \"prev_end\") >= 1),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# Assign episode ID using cumulative sum\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"episode_id\", F.sum(\"new_episode_flag\").over(window_spec))\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"episode_id_hash\",  md5(concat_ws(\"||\", \"member_id\", \"episode_id\")))\n",
    "\n",
    "\n",
    "####__________________\n",
    "window_spec = Window.partitionBy(\"episode_id_hash\").orderBy(\"min_incurred_date_over_hash_step2\")\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"row_num_adjacent_IP\", F.row_number().over(window_spec))\n",
    "\n",
    "IP_svc_hash_dedup = IP_svc_hash.filter(col(\"row_num_adjacent_IP\") == 1).distinct()\n",
    "IP_svc_hash_dedup = IP_svc_hash_dedup.select(\"member_id\", \"detailed_service_category_override\", \"IP_level_II_override\",\"episode_id_hash\", \"svc_overide_hash\")\n",
    "\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.join (\n",
    "    IP_svc_hash_dedup.select( \"episode_id_hash\", \"detailed_service_category_override\", \"IP_level_II_override\").withColumnRenamed(\"detailed_service_category_override\", \"detailed_service_category_override_final\").withColumnRenamed(\"IP_level_II_override\", \"IP_level_II_override_final\"), \n",
    "    on=\"episode_id_hash\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "### Calc final admit / discharge\n",
    "window_spec = Window.partitionBy(\"episode_id_hash\")\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"final_IP_admit\", F.min(\"min_incurred_date_over_hash_step2\").over(window_spec))\n",
    "\n",
    "IP_svc_hash = IP_svc_hash.withColumn(\"final_IP_discharge\", F.max(\"running_max_service_to\").over(window_spec))\n",
    "\n",
    "dedup_df = dedup_df.join(\n",
    "    IP_svc_hash.select(\"svc_overide_hash\", \"detailed_service_category_override_final\", \"IP_level_II_override_final\", \"final_IP_admit\", \"final_IP_discharge\"),\n",
    "    on=\"svc_overide_hash\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "### drop extra columns\n",
    "dedup_df = dedup_df.drop(\"hash_key_temp_step3\", \"IP_level_II_override\", \"detailed_service_category_override\", \"min_IP_Hierarch_over_hash_step2\", \"min_incurred_date_over_hash_step2\", \"running_max_service_to\", \"hash_key_temp_step2\",\"svc_overide_hash\", \"detailed_service_category_temp\", \"hash_key_temp\", \"detailed_service_category_temp\thash_key_temp\tIP_DSC_Hierarchy\")\n",
    "\n",
    "\n",
    "## clean up steps\n",
    "### there is a very small amount of rows that sometime get categorized as NULL -- not sure why; but it was ~ < 0.1% of IP acute medical spend after runnning a number of times. \n",
    "dedup_df = (\n",
    "    dedup_df\n",
    "    .withColumn(\n",
    "        \"detailed_service_category_override_final\",\n",
    "        coalesce(col(\"detailed_service_category_override_final\"), lit(\"Medical\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"IP_level_II_override_final\",\n",
    "        coalesce(col(\"IP_level_II_override_final\"), lit(\"Ungrouped\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"final_IP_admit\",\n",
    "        coalesce(col(\"final_IP_admit\"), col(\"incurred_date\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"final_IP_discharge\",\n",
    "        coalesce(col(\"final_IP_discharge\"), col(\"service_to_date\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"IP_detailed_svccat_errorflag\",\n",
    "        when(col(\"detailed_service_category_override_final\").isNull(), lit(True)).otherwise(lit(False))\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "target_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/commercial/medical/ms_comm_medclaims_Hospice.parquet\" \n",
    "dedup_df.write.parquet(target_path, mode=\"overwrite\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f038fda-b23a-4890-aa81-b2caae0398b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Union the IP Tables; Add Util count field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "475e57f1-ff0b-42f7-a80d-5dd2c05cb6b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hospice = spark.read.parquet(\"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/commercial/medical/ms_comm_medclaims_Hospice.parquet\")\n",
    "\n",
    "df_snf = spark.read.parquet(\"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/commercial/medical/ms_comm_medclaims_SNF.parquet\")\n",
    "\n",
    "df_ipacute = spark.read.parquet(\"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/commercial/medical/ms_comm_medclaims_IPAcute.parquet\")\n",
    "\n",
    "union_df = df_hospice.unionByName(df_snf).unionByName(df_ipacute)\n",
    "\n",
    "\n",
    "union_df = union_df.withColumn(\n",
    "    \"service_cat_id\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"detailed_service_category_override_final\", \"final_IP_admit\")))\n",
    "\n",
    "\n",
    "union_df = union_df.withColumn(\"svc_cat_id_row_num\", F.row_number().over(Window.partitionBy(\"service_cat_id\").orderBy(\"claim_number\", \"seqnum\")))\n",
    "\n",
    "window_spec = Window.partitionBy(\"service_cat_id\")\n",
    "\n",
    "union_df = union_df.withColumn(\"util_count\", \n",
    "                              F.when((F.col(\"svc_cat_id_row_num\") == 1) & (F.sum(\"allowed\").over(window_spec) > 0), 1)\n",
    "                              .when((F.col(\"svc_cat_id_row_num\") == 1) & (F.sum(\"allowed\").over(window_spec) < 0), -1)\n",
    "                              .when((F.col(\"svc_cat_id_row_num\") == 1) & (F.sum(\"allowed\").over(window_spec) == 0), 0)\n",
    "                              .otherwise(None))\n",
    "\n",
    "union_df = union_df.drop(\"Prof_level_II\", \"Prof_level_III\", \"OP_Level_III\", \"IP_DSC_Hierarchy\", \"svc_cat_id_row_num\", \"IP_level_II\", \"IP_level_III\").withColumnRenamed(\"detailed_service_category_override_final\", \"detailed_service_category\").withColumnRenamed(\"IP_level_II_override_final\", \"IP_admit_clinical_label\").withColumnRenamed(\"final_IP_admit\", \"lockton_admit\").withColumnRenamed(\"final_IP_discharge\", \"lockton_discharge\")\n",
    "\n",
    "union_df = union_df.withColumn(\"subservice_category\", \n",
    "                              F.when((F.col(\"detailed_service_category\") == \"SNF\"), \"SNF\")\n",
    "                              .when((F.col(\"detailed_service_category\") == \"Hospice\"), \"Hospice\")\n",
    "                              .otherwise(\"Acute\"))\n",
    "\n",
    "\n",
    "\n",
    "########### Test\n",
    "\n",
    "target_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/commercial/medical/ms_comm_medclaims_IP_Split.parquet\"\n",
    "union_df_IP = union_df.write.partitionBy(\"YEAR\", \"MS_Source_File\").parquet(target_path, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf353a1f-e518-4b7a-bc8d-ce0a7ad0eb2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "OP Hierarchy and Observation Mapping; Add Util Field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffc70d0e-438e-4981-bb49-969241dcc8f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####### Set Claims Data, Filter to OP Facility\n",
    "claims_source = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/commercial/medical/ms_commercial_medical.parquet\" \n",
    "df = spark.read.parquet(claims_source)\n",
    "\n",
    "\n",
    "############### Logic for everything but HH -- HH happens outside of a OP facility event (ex. ER admit leading surgery for example)\n",
    "\n",
    "df = df.filter((col(\"major_service_category\") == \"Outpatient Facility\") & \n",
    "               (~col(\"OP_level_III\").isin([\"Home Health\"])))\n",
    "\n",
    "df = df.withColumn(\"OP_Hierarchy\", \n",
    "                   when(col(\"OP_level_III\") == \"Outpatient Surgery\", 1)\n",
    "                   .when(col(\"OP_level_III\") == \"Observation\", 2)\n",
    "                   .when(col(\"OP_level_III\") == \"Emergency Room\", 3)\n",
    "                   .otherwise(4))\n",
    "\n",
    "window_spec = Window.partitionBy(\"incurred_date\", \"member_id\")\n",
    "df = df.withColumn(\"Min_Hierarchy\", F.min(\"OP_Hierarchy\").over(window_spec))\n",
    "\n",
    "df = df.withColumn(\"Remap_OP\",\n",
    "                   when (col(\"Min_Hierarchy\") == 1, \"Outpatient Surgery\")\n",
    "                   .when (col(\"Min_Hierarchy\") == 2, \"Observation\")\n",
    "                   .when (col(\"Min_Hierarchy\") == 3, \"Emergency Room\")\n",
    "                   .otherwise(col(\"OP_level_III\")))\n",
    "\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"incurred_date_hash\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"incurred_date\")))\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"service_to_date_hash\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"service_to_date\")))\n",
    "\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"incurred_date_hash_svc\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"Remap_OP\", \"incurred_date\")))\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"service_to_date_hash_svc\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"Remap_OP\", \"service_to_date\")))\n",
    "\n",
    "\n",
    "##### multi day observation happens iwth rev vode 762 -- allow for service cat ID to be defined by grouping across days if multi day observation\n",
    "multiday_obs = df.filter(col(\"revenue_code\") == '0762').select(\"member_id\", \"revenue_code\", \"service_to_date_hash_svc\").withColumn(\"multiday_obs_flag\", lit(1)).distinct()\n",
    "\n",
    "##### define svc cat ID here - based on incurred date and remapped svc category\n",
    "df = df.join(\n",
    "    multiday_obs.select(\"service_to_date_hash_svc\", \"multiday_obs_flag\"),\n",
    "    on=\"service_to_date_hash_svc\",\n",
    "    how=\"left\"\n",
    ").withColumn(\n",
    "    \"service_cat_id\",\n",
    "    when(\n",
    "        col(\"multiday_obs_flag\").isNotNull(),\n",
    "        col(\"service_to_date_hash_svc\")\n",
    "    ).otherwise(col(\"incurred_date_hash_svc\"))\n",
    ")\n",
    "\n",
    "\n",
    "##### svc cat ID defined, so now can calculate lockton admit and discharge date (controlling definition here) -- I.e. OP facility by def has 1 date of service, except multiday observation; in final step allowing for HH to be across dates too by letting it's service to date flow through \n",
    "\n",
    "### create lockton admit and discharge\n",
    "window_spec = Window.partitionBy(\"service_cat_id\")\n",
    "df = df.withColumn(\"lockton_admit\", F.min(\"incurred_date\").over(window_spec))\n",
    "df = df.withColumn(\"lockton_discharge\", F.max(\"service_to_date\").over(window_spec))\n",
    "df = df.withColumnRenamed(\"Remap_OP\", \"detailed_service_category_final\")\n",
    "df_OP = df\n",
    "\n",
    "\n",
    "################## _______________ OP HH _______________ #####\n",
    "df_hh = spark.read.parquet(claims_source)\n",
    "\n",
    "df_hh = df_hh.filter((col(\"major_service_category\") == \"Outpatient Facility\") & \n",
    "               (col(\"OP_level_III\").isin([\"Home Health\"])))\n",
    "\n",
    "df_hh = df_hh.withColumn(\"lockton_admit\", coalesce(col(\"admit_date\"), col(\"incurred_date\")))\n",
    "df_hh = df_hh.withColumn(\"lockton_discharge\", coalesce(col(\"discharge_date\"), col(\"incurred_date\")))\n",
    "df_hh = df_hh.withColumnRenamed(\"OP_level_III\", \"detailed_service_category_final\")\n",
    "\n",
    "\n",
    "df_hh = df_hh.withColumn(\n",
    "    \"service_cat_id\",\n",
    "    md5(concat_ws(\"||\", \"member_id\", \"detailed_service_category_final\", \"lockton_admit\")))\n",
    "\n",
    "union_df_OP = df_OP.unionByName(df_hh, allowMissingColumns=True)\n",
    "\n",
    "\n",
    "union_df_OP = union_df_OP.withColumn(\"svc_cat_id_row_num\", F.row_number().over(Window.partitionBy(\"service_cat_id\").orderBy(\"claim_number\", \"seqnum\")))\n",
    "\n",
    "window_spec = Window.partitionBy(\"service_cat_id\")\n",
    "\n",
    "union_df_OP = union_df_OP.withColumn(\"util_count\", \n",
    "                              F.when((F.col(\"svc_cat_id_row_num\") == 1) & (F.sum(\"allowed\").over(window_spec) > 0), 1)\n",
    "                              .when((F.col(\"svc_cat_id_row_num\") == 1) & (F.sum(\"allowed\").over(window_spec) < 0), -1)\n",
    "                              .when((F.col(\"svc_cat_id_row_num\") == 1) & (F.sum(\"allowed\").over(window_spec) == 0), 0)\n",
    "                              .otherwise(None))\n",
    "\n",
    "\n",
    "\n",
    "union_df_OP = union_df_OP.drop(\"service_to_date_hash_svc\", \"IP_level_II\", \"IP_level_III\", \"Prof_level_II\", \"Prof_level_III\", \"OP_Level_III\", \"detailed_service_category_temp\", \"hash_key_temp\", \"IP_DSC_Hierarchy\", \"OP_Hierarchy\", \"Min_Hierarchy\", \"incurred_date_hash\", \"service_to_date_hash\", \"incurred_date_hash_svc\", \"svc_cat_id_row_num\").withColumnRenamed(\"detailed_service_category_final\", \"detailed_service_category\")\n",
    "\n",
    "\n",
    "\n",
    "union_df_OP = union_df_OP.withColumn(\"subservice_category\", \n",
    "                              F.when((col(\"detailed_service_category\").isin([\"Emergency Room\", \"Outpatient Surgery\", \"Observation\"])), \"OP Visit\")\n",
    "                              .otherwise(\"OP Service\"))\n",
    "\n",
    "\n",
    "\n",
    "##### Export\n",
    "target_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/commercial/medical/ms_comm_medclaims_OP_Split.parquet\"\n",
    "\n",
    "union_df_OP.write.partitionBy(\"YEAR\", \"MS_Source_File\").parquet(target_path, mode=\"overwrite\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d54ce09-ddd1-4b8e-bc36-f5f6d62edd60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Professional Charges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1245d5d-2977-495a-9d95-66ed9cc3de75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####### Set Claims Data, Filter to OP Facility\n",
    "claims_source = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/commercial/medical/ms_commercial_medical.parquet\" \n",
    "df = spark.read.parquet(claims_source)\n",
    "\n",
    "\n",
    "############### Logic for everything but HH -- HH happens outside of a OP facility event (ex. ER admit leading surgery for example)\n",
    "\n",
    "df_prof = df.filter((col(\"major_service_category\") == \"Professional\"))\n",
    "                    \n",
    "\n",
    "### add svc cat id after renaming DSC, add util count\n",
    "df_prof = df_prof.withColumnRenamed(\"hash_key_temp\", \"service_cat_id\").withColumnRenamed(\"detailed_service_category_temp\", \"detailed_service_category\").withColumnRenamed(\"Prof_level_II\", \"subservice_category\")\n",
    "\n",
    "df_prof = df_prof.withColumn(\"svc_cat_id_row_num\", F.row_number().over(Window.partitionBy(\"service_cat_id\").orderBy(\"claim_number\", \"seqnum\")))\n",
    "\n",
    "window_spec = Window.partitionBy(\"service_cat_id\")\n",
    "\n",
    "df_prof = df_prof.withColumn(\"util_count\", \n",
    "                              F.when((F.col(\"svc_cat_id_row_num\") == 1) & (F.sum(\"allowed\").over(window_spec) > 0), 1)\n",
    "                              .when((F.col(\"svc_cat_id_row_num\") == 1) & (F.sum(\"allowed\").over(window_spec) < 0), -1)\n",
    "                              .when((F.col(\"svc_cat_id_row_num\") == 1) & (F.sum(\"allowed\").over(window_spec) == 0), 0)\n",
    "                              .otherwise(None))\n",
    "\n",
    "\n",
    "df_prof = df_prof.drop(\"IP_level_II\", \"IP_level_III\", \"Prof_level_III\", \"OP_Level_III\", \"IP_DSC_Hierarchy\", \"svc_cat_id_row_num\")\n",
    "                    \n",
    "##### Export\n",
    "target_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/commercial/medical/ms_comm_medclaims_Prof_Split.parquet\"\n",
    "\n",
    "df_prof.write.partitionBy(\"YEAR\", \"MS_Source_File\").parquet(target_path, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d236ac15-dfd8-49da-bb46-debf17bfd121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Final Combination of the Tables and Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15761ddc-d457-4a57-a86f-6a5a9a338be1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "IP_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/commercial/medical/ms_comm_medclaims_IP_Split.parquet\"\n",
    "OP_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/commercial/medical/ms_comm_medclaims_OP_Split.parquet\"\n",
    "Prof_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/commercial/medical/ms_comm_medclaims_Prof_Split.parquet\"\n",
    "\n",
    "\n",
    "IP = spark.read.parquet(IP_path)\n",
    "OP = spark.read.parquet(OP_path)\n",
    "Prof = spark.read.parquet(Prof_path)\n",
    "\n",
    "Combined_table = IP.unionByName(OP, allowMissingColumns=True).unionByName(Prof, allowMissingColumns=True)\n",
    "\n",
    "\n",
    "# Define the desired order of columns\n",
    "column_order = [\"procedure_code\",\n",
    "\"DRG\",\n",
    "\"fachdid\",\n",
    "\"member_id\",\n",
    "\"claim_number\",\n",
    "\"seqnum\",\n",
    "\"marketscan_version_number\",\n",
    "\"family_id\",\n",
    "\"principal_diagnosis\",\n",
    "\"icd_diagnosis_code1\",\n",
    "\"icd_diagnosis_code2\",\n",
    "\"icd_diagnosis_code3\",\n",
    "\"icd_diagnosis_code4\",\n",
    "\"icd_diagnosis_code5\",\n",
    "\"icd_diagnosis_code6\",\n",
    "\"icd_diagnosis_code7\",\n",
    "\"icd_diagnosis_code8\",\n",
    "\"icd_diagnosis_code9\",\n",
    "\"principal_procedure\",\n",
    "\"procedure_code2\",\n",
    "\"procedure_code3\",\n",
    "\"procedure_code4\",\n",
    "\"procedure_code5\",\n",
    "\"procedure_code6\",\n",
    "\"cpt_modifier\",\n",
    "\"bill_type\",\n",
    "\"admit_type\",\n",
    "\"discharge_status\",\n",
    "\"revenue_code\",\n",
    "\"cob\",\n",
    "\"coins\",\n",
    "\"copay\",\n",
    "\"deduct\",\n",
    "\"paid\",\n",
    "\"allowed\",\n",
    "\"quantity\",\n",
    "\"units\",\n",
    "\"paid_inn\",\n",
    "\"provider_id\",\n",
    "\"npi\",\n",
    "\"service_to_date\",\n",
    "\"paid_date\",\n",
    "\"incurred_date\",\n",
    "\"discharge_date\",\n",
    "\"admit_date\",\n",
    "\"dob_year\",\n",
    "\"member_age\",\n",
    "\"cap_svc\",\n",
    "\"proctyp\",\n",
    "\"dxver\",\n",
    "\"facprof\",\n",
    "\"mhsacovg\",\n",
    "\"ntwkprov\",\n",
    "\"plan_type\",\n",
    "\"proc_group\",\n",
    "\"service_category\",\n",
    "\"MDC\",\n",
    "\"employee_region\",\n",
    "\"employee_msa\",\n",
    "\"pos\",\n",
    "\"provider_type\",\n",
    "\"data_type\",\n",
    "\"age_group\",\n",
    "\"employee_class\",\n",
    "\"employee_status\",\n",
    "\"employee_geo\",\n",
    "\"eidflag\",\n",
    "\"employee_relation\",\n",
    "\"enrflag\",\n",
    "\"physician_specialty\",\n",
    "\"rx\",\n",
    "\"member_gender\",\n",
    "\"healthplan_vs_employer\",\n",
    "\"industry\",\n",
    "\"medicare_advantage_flag\",\n",
    "\"msn_version\",\n",
    "\"caseid\",\n",
    "\"major_service_category\",\n",
    "\"subservice_category\",\n",
    "\"detailed_service_category\",\n",
    "\"service_cat_id\",\n",
    "\"util_count\",\n",
    "\"IP_admit_clinical_label\",\n",
    "\"lockton_admit\",\n",
    "\"lockton_discharge\",\n",
    "\"IP_detailed_svccat_errorflag\",\n",
    "\"multiday_obs_flag\",\n",
    "\"YEAR\",\n",
    "\"MS_Source_File\"]\n",
    "\n",
    "# Reorder the columns in Combined_table\n",
    "Combined_table = Combined_table.select([col(c) for c in column_order])\n",
    "\n",
    "\n",
    "\n",
    "target_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/commercial/medical_final/ms_comm_medclaims_combined.parquet\"\n",
    "Combined_table.write.partitionBy(\"YEAR\", \"MS_Source_File\").parquet(target_path, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b7f6210-05b8-419c-93bf-49ee84c074d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rx Service Categories -- Add Rx Tiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a3dd616-3f5e-445f-b016-34468fec7159",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "Rx_path = \"wasbs://marketscan@rgdevglobalreahl.blob.core.windows.net/marketscan/commercial/marketscan-Rx-preprod-step2\"\n",
    "Rx = spark.read.parquet(Rx_path)\n",
    "\n",
    "Rx = Rx.withColumn(\"major_service_category\", lit(\"Rx\"))\n",
    "\n",
    "Rx = Rx.withColumn(\n",
    "    \"subservice_category\",\n",
    "    F.when(F.abs(col(\"allowed\")) > 2500, \"Specialty\")\n",
    "    .when(col(\"mail_order_ind\").isin([\"1\"]), \"Retail\")\n",
    "    .when(col(\"mail_order_ind\").isin([\"2\"]), \"Mail Order\")\n",
    "    .when(F.abs(col(\"days_supply\")) >= 84, \"Mail Order\")\n",
    "    .otherwise(\"Retail\"))\n",
    "\n",
    "\n",
    "Rx = Rx.withColumn(\n",
    "    \"detailed_service_category\",\n",
    "    F.when(F.abs(col(\"allowed\")) > 2500, \"Specialty\")\n",
    "    .when(col(\"generic_equivalent\").isin([\"1\"]), \"Single-Source Brand\")\n",
    "    .when(col(\"generic_equivalent\").isin([\"2\", \"3\"]), \"Multi-Source Brand\")\n",
    "    .when(col(\"generic_equivalent\").isin([\"4\", \"5\"]), \"Generic\")\n",
    "    .otherwise(\"OTC/Other\"))\n",
    "\n",
    "\n",
    "target_path = \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/commercial/Rx/ms_comm_Rxclaims_combined.parquet\"\n",
    "Rx.write.partitionBy(\"year\" ).parquet(target_path, mode=\"overwrite\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0a714dc-1bdc-47b5-8683-700e8b0ce2ef",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"#row_number#\":52},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752090065378}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>major_service_category</th><th>subservice_category</th><th>detailed_service_category</th><th>allowed</th><th>days</th><th>scripts</th></tr></thead><tbody><tr><td>Rx</td><td>Mail Order</td><td>Generic</td><td>8.319808343799938E8</td><td>1173006462</td><td>14393590</td></tr><tr><td>Rx</td><td>Mail Order</td><td>Multi-Source Brand</td><td>3.466670897499987E8</td><td>75832580</td><td>909996</td></tr><tr><td>Rx</td><td>Mail Order</td><td>OTC/Other</td><td>2.793562312699998E8</td><td>91258644</td><td>1162316</td></tr><tr><td>Rx</td><td>Mail Order</td><td>Single-Source Brand</td><td>2.3962821795700116E9</td><td>146638742</td><td>2016712</td></tr><tr><td>Rx</td><td>Retail</td><td>Generic</td><td>5.492812964031028E9</td><td>6243018322</td><td>171144261</td></tr><tr><td>Rx</td><td>Retail</td><td>Multi-Source Brand</td><td>1.5491423255799906E9</td><td>143162145</td><td>5253334</td></tr><tr><td>Rx</td><td>Retail</td><td>OTC/Other</td><td>8.652890789100323E8</td><td>324055658</td><td>8218353</td></tr><tr><td>Rx</td><td>Retail</td><td>Single-Source Brand</td><td>1.553440154778994E10</td><td>775143418</td><td>23210757</td></tr><tr><td>Rx</td><td>Specialty</td><td>Specialty</td><td>6.552138166567003E10</td><td>335856905</td><td>8489387</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Rx",
         "Mail Order",
         "Generic",
         8.319808343799938E8,
         "1173006462",
         14393590
        ],
        [
         "Rx",
         "Mail Order",
         "Multi-Source Brand",
         3.466670897499987E8,
         "75832580",
         909996
        ],
        [
         "Rx",
         "Mail Order",
         "OTC/Other",
         2.793562312699998E8,
         "91258644",
         1162316
        ],
        [
         "Rx",
         "Mail Order",
         "Single-Source Brand",
         2.3962821795700116E9,
         "146638742",
         2016712
        ],
        [
         "Rx",
         "Retail",
         "Generic",
         5.492812964031028E9,
         "6243018322",
         171144261
        ],
        [
         "Rx",
         "Retail",
         "Multi-Source Brand",
         1.5491423255799906E9,
         "143162145",
         5253334
        ],
        [
         "Rx",
         "Retail",
         "OTC/Other",
         8.652890789100323E8,
         "324055658",
         8218353
        ],
        [
         "Rx",
         "Retail",
         "Single-Source Brand",
         1.553440154778994E10,
         "775143418",
         23210757
        ],
        [
         "Rx",
         "Specialty",
         "Specialty",
         6.552138166567003E10,
         "335856905",
         8489387
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 17
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "major_service_category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "subservice_category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "detailed_service_category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "allowed",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "days",
         "type": "\"decimal(38,0)\""
        },
        {
         "metadata": "{}",
         "name": "scripts",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\t\t\t\t\t\t\t\t\n",
    "\n",
    "DROP VIEW IF EXISTS LDS_view;\t\t\t\t\t\t\t\t\n",
    "CREATE TEMPORARY VIEW LDS_view \t\t\t\t\t\t\t\t\n",
    "USING PARQUET\t\t\t\t\t\t\t\t\n",
    "OPTIONS (\t\t\t\t\t\t\t\t\n",
    " \t\t\t\t\t\t\t\t\n",
    " path \"wasbs://ms-etl-preprod-servicecat@rgdevglobalreahl.blob.core.windows.net/commercial/Rx/ms_comm_Rxclaims_combined.parquet\"\n",
    " \t\t\t\t\t\t\t\t\n",
    " );\t\t\t\t\t\t\t\t\n",
    "\n",
    "\n",
    "-- \"wasbs://marketscan@rgdevglobalreahl.blob.core.windows.net/marketscan/medicare/MDCR_D\"\n",
    "-- \"wasbs://marketscan@rgdevglobalreahl.blob.core.windows.net/marketscan/commercial/marketscan-Rx-preprod-step2\"\n",
    "\n",
    "select major_service_category,\n",
    "subservice_category,\n",
    "detailed_service_category,\n",
    "-- mail_order_ind,\n",
    "sum(allowed) as allowed,\n",
    "sum(days_supply) as days,\n",
    "count(*) as scripts\n",
    "from LDS_view\n",
    "group by 1,2,3\n",
    "order by 1,2,3\n",
    "\n",
    "limit 100\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3879550410129494,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Service_Category_Logic_MS_Comm",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}